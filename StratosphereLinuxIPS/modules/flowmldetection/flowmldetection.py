from slips_files.common.abstracts import Module
import multiprocessing
from slips_files.core.database.database import __database__
from slips_files.common.config_parser import ConfigParser
from sklearn.preprocessing import StandardScaler
from slips_files.common.slips_utils import utils
from modules.flowmldetection.connection_4_tuple import Connection4tuple
import threading
import pandas as pd
import json
import datetime
import traceback
import os
import sys
import time
import xgboost as xgb
import numpy as np
import redis
from .labeling_conn_log import take_label_from_binet_flow
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn import preprocessing
import joblib
import signal
import asyncio

# This horrible hack is only to stop sklearn from printing those warnings
def warn(*args, **kwargs):
    pass

import warnings
warnings.warn = warn

class Module(Module, multiprocessing.Process):
    # Name: short name of the module. Do not use spaces
    name = 'Flow ML Detection'
    description = (
        'Train or test a Machine Learning model to detect malicious flows'
    )
    authors = ['Sebastian Garcia']

    def __init__(self, outputqueue, redis_port,output_dir, filepath):
        multiprocessing.Process.__init__(self)
        self.outputqueue = outputqueue
        __database__.start(redis_port)
        # Subscribe to the channel
        self.c1 = __database__.subscribe('new_flow')
        self.fieldseparator = __database__.getFieldSeparator()
        # Set the output queue of our database instance
        __database__.setOutputQueue(self.outputqueue)
        # Read the configuration
        self.read_configuration()
        # Minum amount of new lables needed to trigger the train
        self.minimum_lables_to_retrain = 50
        # To plot the scores of training
        # self.scores = []
        # The scaler trained during training and to use during testing
        self.output_dir=output_dir
        self.zeek_folder=os.path.join(output_dir,'zeek_files')
        self.connection_folder=os.path.join(output_dir,'Connection_4-tuples')
        self.scaler = StandardScaler()
        self.connection_4_tuples=dict() #all index of connection 4-tuple in dataset
        self.conn_dict=dict() #all conn record not belong to connection 4-tuple
        self.ssl_dict=dict() #all ssl record not belong to connection 4-tuple
        self.x509_dict=dict() #all x509 record not belong to connection 4-tuple
        self.file_path=filepath

    def read_configuration(self):
        conf = ConfigParser()
        self.mode = conf.get_ml_mode()

    def print(self, text, verbose=1, debug=0):
        """
        Function to use to print text using the outputqueue of slips.
        Slips then decides how, when and where to print this text by taking all the processes into account
        :param verbose:
            0 - don't print
            1 - basic operation/proof of work
            2 - log I/O operations and filenames
            3 - log database/profile/timewindow changes
        :param debug:
            0 - don't print
            1 - print exceptions
            2 - unsupported and unhandled types (cases that may cause errors)
            3 - red warnings that needs examination - developer warnings
        :param text: text to print. Can include format like 'Test {}'.format('here')
        """

        levels = f'{verbose}{debug}'
        self.outputqueue.put(f'{levels}|{self.name}|{text}')
### TRAIN
    #label conn log
    def label_dataset(self):
        folder_dataset=self.file_path.rsplit('/', 1)[0]
        take_label_from_binet_flow(folder_dataset, self.zeek_folder)

    #loading conn log
    # def load_conn_label_log(self,r_train_conn):
    def load_conn_label_log(self):
        print('Loading conn log')
        with open(os.path.join(self.zeek_folder,'conn_label.log'), 'r') as conn_file:
            for conn_line in conn_file:
                split=json.loads(conn_line)
                conn_uid=split['uid']
                if not conn_uid in self.conn_dict:
                    split['connection_index']=split['id.orig_h'], split['id.resp_h'], split['id.resp_p'], split['proto']
                    self.conn_dict[conn_uid]=split
                # if not conn_uid in r_train_conn.keys():
                #     print(1111, conn_uid)
                #     split['connection_index']=split['id.orig_h'], split['id.resp_h'], split['id.resp_p'], split['proto']
                #     split['connection_index']=str(split['connection_index'])
                #     del split['local_resp']
                #     del split['local_orig']
                #     r_train_conn.hset(conn_uid, mapping=split)

    # def create_4_tuple(self,r_train_conn):
    def create_4_tuple(self):
        print('Create 4-tuple')
        if os.path.isfile(os.path.join(self.zeek_folder,'ssl.log')):
            with open(os.path.join(self.zeek_folder,'ssl.log')) as ssl_file:
                for ssl_line in ssl_file:
                    if '#' == ssl_line[0]:
                        continue
                    ssl_split=json.loads(ssl_line)
                    if 'version' not in ssl_split:
                        continue
                    ssl_uid=ssl_split['uid']
                    try:
                        conn_split=self.conn_dict[ssl_uid]
                    except:
                        continue
                    connection_index=conn_split['id.orig_h'],conn_split['id.resp_h'],conn_split['id.resp_p'],conn_split['proto']
                    try:
                        self.connection_4_tuples[connection_index].add_ssl_flow_train(conn_split)
                    except:
                        self.connection_4_tuples[connection_index]=Connection4tuple(connection_index)
                        self.connection_4_tuples[connection_index].add_ssl_flow_train(conn_split)
                    valid_x509_list=self.split_ssl(ssl_split)
                    self.connection_4_tuples[connection_index].add_ssl_log(ssl_split, valid_x509_list)
                    # try:
                    #     conn_split=r_train_conn.hgetall(ssl_uid)
                    # except:
                    #     continue
                    # connection_index=conn_split['id.orig_h'],conn_split['id.resp_h'],conn_split['id.resp_p'],conn_split['proto']
                    # try:
                    #     self.connection_4_tuples[connection_index].add_ssl_flow_train(conn_split)
                    # except:
                    #     self.connection_4_tuples[connection_index]=Connection4tuple(connection_index)
                    #     self.connection_4_tuples[connection_index].add_ssl_flow_train(conn_split)
                    # valid_x509_list=self.split_ssl(ssl_split)
                    # self.connection_4_tuples[connection_index].add_ssl_log(ssl_split, valid_x509_list)
            print('connection 4-tuple length: '+str(len(self.connection_4_tuples.keys())))
        else:
            sys.exit()

    #caculate features
    # def process_flows(self, redis, r_train_conn):
    def process_flows(self, redis):
        redis.publish(
                channel='log_train',
                message=json.dumps({'msg':'Start load data log','time':datetime.datetime.now().strftime('%H:%M:%S')})
                # message=json.dumps({'msg':'Start load data log','time':str(datetime.datetime.now().time())})
            )
        self.label_dataset()
        self.load_x509_log()
        # self.load_conn_label_log(r_train_conn)
        self.load_conn_label_log()
        redis.publish(
                channel='log_train',
                message=json.dumps({'msg':'Start extract feature','time':datetime.datetime.now().strftime('%H:%M:%S')})
                # message=json.dumps({'msg':'Start extract feature','time':str(datetime.datetime.now().time())})
            )
        self.create_4_tuple()
        data=[]
        for connection_index in self.connection_4_tuples:
            data.append(self.connection_4_tuples[connection_index].get_feature_in_train())
        # keys = r_train_conn.keys('*')
        # r_train_conn.delete(*keys)
        raw_result=pd.DataFrame.from_dict(data, orient='columns').reset_index()
        raw_result = raw_result.drop('index', axis=1)
        result= self.combine_and_normalize_train(raw_result)
        # result.to_csv(os.path.join(self.output_dir,'dataset.csv'),index=False)
        return result
    
    def combine_and_normalize_train(self,df):
        data=pd.read_csv('./modules/flowmldetection/dataset_test/extract_dataset.csv')
        folder_dataset=self.file_path.rsplit('/', 1)[0]
        df['dataset']=folder_dataset
        dataset=pd.concat([data, df], ignore_index=True)
        dataset.to_csv('./modules/flowmldetection/dataset_test/extract_dataset.csv',mode='w', index=False, index_label=False)
        data=dataset.iloc[:,:-1]
        label=dataset.iloc[:,-1]
        X_train, X_test, y_train, y_test = train_test_split( data, label, test_size=0.2, random_state=42, stratify=label)
        train=pd.concat([X_train, y_train], axis=1, join='inner')
        test=pd.concat([X_test, y_test], axis=1, join='inner')
        train=pd.DataFrame(train)
        test=pd.DataFrame(test)
        normalizer = preprocessing.MaxAbsScaler()
        train.iloc[:,2:-1] = normalizer.fit_transform(train.iloc[:,2:-1])
        for column in train.iloc[:,2:-1].columns.values:
            train.loc[train[column] <0, column] = -1
        scaler_filename = "./modules/flowmldetection/scaler.save"
        joblib.dump(normalizer, scaler_filename)
        test.iloc[:,2:-1] = normalizer.transform(test.iloc[:,2:-1])
        for column in test.iloc[:,2:-1].columns.values:
            test.loc[test[column] <0, column] = -1
        train.to_csv('./modules/flowmldetection/dataset_test/trainSet.csv',mode='w', index=False, index_label=False)
        test.to_csv('./modules/flowmldetection/dataset_test/testSet.csv',mode='w', index=False, index_label=False)
        return {'train':train.to_dict('records'),'test':test.to_dict('records')}
    
    def trainModel(self,dataset, redis):
        redis.publish(
                channel='log_train',
                message=json.dumps({'msg':'Start training','time':datetime.datetime.now().strftime('%H:%M:%S')})
                # message=json.dumps({'msg':'Start training','time':str(datetime.datetime.now().time())})
            )
        ###START: for update model only new file pcap###
        # if len(dataset['label'].unique().tolist())<2:
        #     if 1 in dataset['label'].unique().tolist():
        #         df={'connection_index': "('10.0.2.15', '91.198.174.192', '443', 'tcp')", 'number_of_connection_records': 0.0002477086945751, 'mean_of_duration': 0.0028955690341913, 'standard_deviation_of_duration': 0.0043731311854657, 'standard_deviation_range_duration': 0.3333333333333333, 'payload_bytes_from_originator': 0.0003860377407015, 'payload_bytes_from_responder': 0.000224024587471, 'ratio_of_responder_bytes': 0.98110762931936, 'ratio_established_states': 1.0, 'inbound_packets': 0.0003361830082611, 'outbound_packets': 0.0003434911229362, 'periodicity_mean': 0.0002114648075046, 'standart_deviation_of_periodicity': 0.0010166964836078, 'ratio_connection_record_ssl_aggregations': 0.0, 'ratio_tls_ssl': 1.0, 'ratio_sni': 1.0, 'sni_as_ip': 1.0, 'mean_certificate_path': 0.2222222222222222, 'public_key_mean': 0.0625, 'mean_certificate_validatity_periods': 0.0172727272727272, 'standard_deviation_certificate_validatity_periods': 0.0, 'validity_certificate_period_during_capturing': 0.0, 'mean_age_of_cert': 0.0152852633486915, 'amount_of_certificates': 0.0714285714285714, 'mean_number_of_domains_in_SAN_DNS': 0.0464326160815402, 'ratio_certificate_records_SSL_records': 1.0, 'sni_in_san_dns': 1.0, 'cn_in_san_dns': 1.0, 'label': 0}
        #         dataset=pd.concat([dataset,pd.DataFrame([df])], ignore_index=True)
        #     # else:
        #     #     df={'connection_index': "('147.32.84.170', '91.189.89.60', '443', 'tcp')", 'number_of_connection_records': 0.0011559739080175, 'mean_of_duration': 2.0253727608379493e-05, 'standard_deviation_of_duration': 8.585726728529634e-05, 'standard_deviation_range_duration': 0.0238095238095238, 'payload_bytes_from_originator': 0.0012351921792467, 'payload_bytes_from_responder': 8.693767114265156e-05, 'ratio_of_responder_bytes': 0.862985710809609, 'ratio_established_states': 1.0, 'inbound_packets': 0.0002343408604726, 'outbound_packets': 0.0004743981632735, 'periodicity_mean': 4.807158177251948e-06, 'standart_deviation_of_periodicity': 0.0003408502623316, 'ratio_connection_record_ssl_aggregations': 0.0, 'ratio_tls_ssl': -1.0, 'ratio_sni': 0.0, 'sni_as_ip': 1.0, 'mean_certificate_path': -1.0, 'public_key_mean': -1.0, 'mean_certificate_validatity_periods': -1.0, 'standard_deviation_certificate_validatity_periods': -1.0, 'validity_certificate_period_during_capturing': 0.0, 'mean_age_of_cert': -1.0, 'amount_of_certificates': 0.0, 'mean_number_of_domains_in_SAN_DNS': -1.0, 'ratio_certificate_records_SSL_records': 0.0, 'sni_in_san_dns': -1.0, 'cn_in_san_dns': -1.0, 'label': 1}
        #     #     dataset=pd.concat([dataset,pd.DataFrame([df])], ignore_index=True)
        # trainLabel=dataset.iloc[:,-1]
        # trainData=dataset.iloc[:,1:-1]
        # X_train, X_test, y_train, y_test = train_test_split( trainData, trainLabel, test_size=0.2, random_state=42, stratify=trainLabel)
        ###END###

        ###START: update all data
        testSet=pd.DataFrame(dataset['test'])
        trainSet=pd.DataFrame(dataset['train'])
        trainLabel=trainSet.iloc[:,-1]
        trainData=trainSet.iloc[:,2:-1] 
        testLabel=testSet.iloc[:,-1]
        testData=testSet.iloc[:,2:-1]
        ###END###
        xgb_classifier = xgb.XGBClassifier(n_estimators=1000, max_depth=3, gamma=0.1, min_child_weight=5)
        # xgb_classifier.fit(X_train,y_train,xgb_model='./modules/flowmldetection/model_xgb.bin')
        # xgb_classifier.fit(X_train,y_train,xgb_model='./modules/flowmldetection/model_xgb_test.bin')
        xgb_classifier.fit(trainData,trainLabel)
        y_pred = xgb_classifier.predict(testData)
        print('-------------------------')
        print("Accuracy:",metrics.accuracy_score(testLabel, y_pred))
        print("Precision score: ",metrics.precision_score(testLabel, y_pred))
        print("Recall score: ",metrics.recall_score(testLabel, y_pred))
        print("F1 score: ",metrics.f1_score(testLabel, y_pred))
        print('-------------------------')
        # xgb_classifier.save_model('./modules/flowmldetection/model_xgb.bin')
        xgb_classifier.save_model('./modules/flowmldetection/model_xgb_test.bin')
        redis.publish(
                channel='log_train',
                message=json.dumps({'msg':'Done training','time':datetime.datetime.now().strftime('%H:%M:%S'),'acc':str(metrics.accuracy_score(testLabel, y_pred))})
            )
        return 

### TEST
    #loading conn.log, add connection record to conn_dict
    def load_conn_log(self):
        with open(os.path.join(self.zeek_folder,'conn.log'), 'r') as conn_file:
            for conn_line in conn_file:
                split=json.loads(conn_line)
                conn_uid=split['uid']
                if not conn_uid in self.conn_dict:
                    split['connection_index']=split['id.orig_h'], split['id.resp_h'], split['id.resp_p'], split['proto']
                    self.conn_dict[conn_uid]=split
            # conn_file.close()

    #loading ssl.log, add ssl record to ssl_dict
    def load_ssl_log(self):
        if not os.path.isfile(os.path.join(self.zeek_folder,'ssl.log')):
            return
        with open(os.path.join(self.zeek_folder,'ssl.log'), 'r') as ssl_file:
            for ssl_line in ssl_file:
                split=json.loads(ssl_line)
                ssl_uid=split['uid']
                if not ssl_uid in self.ssl_dict:
                    self.ssl_dict[ssl_uid]=split
            # ssl_file.close()

    #loading x509.log, add x509 record to x509_dict
    def load_x509_log(self):
        if not os.path.isfile(os.path.join(self.zeek_folder,'x509.log')):
            return
        with open(os.path.join(self.zeek_folder,'x509.log'), 'r') as x509_file:
            for x509_line in x509_file:
                split=json.loads(x509_line)
                x509_uid=split['fingerprint']
                if not x509_uid in self.x509_dict:
                    self.x509_dict[x509_uid]=split
            # x509_file.close()

    #caculate features
    def process_features(self, connection_index):
        connection_4_tuple=Connection4tuple(connection_index)
        str_connection_index=';'.join(str(x) for x in connection_index)
        caculate_conn_feature = threading.Thread(target=self.caculate_conn_feature, args=(connection_4_tuple, str_connection_index,), name='Caculating connection features')
        caculate_ssl_feature = threading.Thread(target=self.caculate_ssl_feature, args=(connection_4_tuple, str_connection_index,),name='Caculating ssl and x509 features')
        caculate_conn_feature.start()
        caculate_ssl_feature.start()
        caculate_conn_feature.join()
        caculate_ssl_feature.join()
        data=connection_4_tuple.get_feature()
        # data['connection_index']=''+data['connection_index']
        data=pd.DataFrame([data])
        self.write_feature_connection_to_csv(connection_index, data)
        normalize=self.normalize_dataset(data)
        return normalize

    #save to dataset.csv  
    def write_feature_connection_to_csv(self,connection_index, data):
        result=pd.DataFrame()
        if os.path.isfile(os.path.join(self.output_dir,'dataset.csv')):
            result=pd.read_csv(os.path.join(self.output_dir,'dataset.csv'))
            result = result[result['connection_index'] != str(connection_index)]
        result = pd.concat([result, data], ignore_index=True)
        #result=self.normalize_dataset(result)
        #result.insert(pd.DataFrame.from_dict(data))
        result.to_csv(os.path.join(self.output_dir,'dataset.csv'), index=False,)
        return result
    
    #data preprocessing
    def normalize_dataset(self, df):
        normalizer = joblib.load('./modules/flowmldetection/scaler.save')
        df.iloc[:,1:] = normalizer.transform(df.iloc[:,1:])
        for column in df.iloc[:,1:].columns.values:
            df.loc[df[column] <0, column] = -1 
        print(1111, df)
        return df
        # df = df.reset_index(drop=True)
        # conn=df.loc[df['connection_index'] == connection_index]
        # for col in df.columns:
        #     if col in ['connection_index']:
        #         continue
        #     df[col] = df[col].astype('float64')
        #     max=df[col].max()
        #     if max != 0:
        #         for index, row in conn.iterrows():
        #             if row[col]!=-1:
        #                 conn.loc[index,col]=np.float64(row[col]/max)
        #         #if connection_feature[col]
        #         #df.loc[df[col] != -1, col] = np.float64(df[col]/max)
        #         # for ro in range(connection_feature.shape[0]):
        #         #     if df[col].values[ro]!=-1:
        #         #         df[col].values[ro]=np.float64(df[col].values[ro]/max)
        # return conn
    
    #caculate connection features
    def caculate_conn_feature(self,connection_4_tuple, connection_index):
        with open(os.path.join(self.connection_folder,connection_index,'conn.log'), 'r') as conn_file:
            for conn_line in conn_file:
                conn_line=json.loads(conn_line)
                connection_4_tuple.add_ssl_flow(conn_line)
            conn_file.close()

    #caculate ssl and x509 feature
    def caculate_ssl_feature(self,connection_4_tuple, connection_index):
        with open(os.path.join(self.connection_folder,connection_index,'ssl_x509.log'), 'r') as ssl_x509_file:
            for ssl_x509_line in ssl_x509_file:
                ssl_x509_line=json.loads(ssl_x509_line)
                ssl_line=ssl_x509_line['ssl']
                x509_list=ssl_x509_line['x509']
                connection_4_tuple.add_ssl_log(ssl_line, x509_list,)
            ssl_x509_file.close()
    
    #writing conn record in connection 4-tuple to folder Connection 4-tuple
    def write_conn_log(self, list_conn_record, connection_index):
        with open(os.path.join(self.connection_folder,connection_index,'conn.log'), 'a') as conn_file:
            for conn_line in list_conn_record:
                conn_file.write(json.dumps(conn_line)+"\r\n")
                # self.conn_dict.pop(conn_line['uid'])
            conn_file.close()
        list_uid=[d['uid'] for d in list_conn_record]
        [self.conn_dict.pop(key) for key in list_uid]        

    #writing ssl record, x509 record in connection 4-tuple to folder Connection 4-tuple
    def write_ssl_x509_log(self, ssl_record, x509_record, connection_index):
        data=dict()
        data['ssl']=ssl_record
        data['x509']=x509_record
        with open(os.path.join(self.connection_folder,connection_index,'ssl_x509.log'), 'a') as ssl_file:
            ssl_file.write(json.dumps(data)+ "\r\n")
            ssl_file.close()
        self.ssl_dict.pop(ssl_record['uid'])
        if x509_record and x509_record[0] is not None:
            self.x509_dict.pop(x509_record[0]['fingerprint'])

    def get_x509_line(self, x509_uids_list):
        x509_line=None
        uid_x509=x509_uids_list[0]
        t=1
        while t<=6:
            try:
                if t>5:
                    print("Error: [get_x509_lines] In ProcessLogs.py x509 does not have this x509uid: "+uid_x509)
                    return None
                #if self.x509_dict[uid_x509]:
                x509_line=self.x509_dict[uid_x509]
                if x509_line:
                    # for x509 in x509_uids_list:
                    #     del self.x509_dict[x509]
                    break
            except:
                t+=1
                print("Retry waiting x509 record uid: "+uid_x509+" for 3s")
                time.sleep(1)
        return x509_line
    
    #get list x509 cert
    def split_ssl(self, ssl_line):
        if ('cert_chain_fps' not in ssl_line) or ('-'==ssl_line['cert_chain_fps'] or '(object)'==ssl_line['cert_chain_fps']):
            print("SSL agregation not have x509 certificate")
            return []
        return [self.get_x509_line(ssl_line['cert_chain_fps'])]
    
    def create_folder_connection(self, connection_index):
        # self.sio.emit('logs', { 'name':'create folder connection'})
        if not os.path.exists(self.connection_folder):
            os.mkdir(self.connection_folder)
        os.mkdir(os.path.join(self.connection_folder,connection_index))

    #validate ip address
    def validate_ip(self,s):
        a = s.split('.')
        if len(a) != 4:
            return False
        for x in a:
            if not x.isdigit():
                return False
            i = int(x)
            if i < 0 or i > 255:
                return False
        return True

    def process_flow(self):
        """
        Process one flow. Only used during detection in testing
        Store the pandas df in self.flow
        """
        try:
            #read data flow, get connection index
            raw_flow = self.flow_dict
            connection_index=raw_flow['saddr'], raw_flow['daddr'], raw_flow['dport'], raw_flow['proto']
            #start multithreading for read conn.log, ssl.log, x509.log
            read_conn_log = threading.Thread(target=self.load_conn_log, name='Loading file conn.log')
            read_ssl_log = threading.Thread(target=self.load_ssl_log, name='Loading file ssl.log')
            read_x509_log = threading.Thread(target=self.load_x509_log, name='Loading file x509.log')
            read_conn_log.start()
            read_ssl_log.start()
            read_x509_log.start()
            #get conn, ssl and x509 record and caculate feature
            while True:
                try:
                    conn_log=self.conn_dict[raw_flow['uid']]
                    break
                except:
                    time.sleep(0.1) #if flow not exist yet in conn.log
            try:
                ssl_log=self.ssl_dict[raw_flow['uid']]
            except:
                ssl_log=None
            conn_logs=[]
            str_connection_index=';'.join(str(x) for x in connection_index)
            if ssl_log is not None and 'version' in ssl_log:
                if not connection_index in self.connection_4_tuples:
                    self.create_folder_connection(str_connection_index)
                    self.connection_4_tuples[connection_index]=Connection4tuple(connection_index)
                    for conn in self.conn_dict:
                        if self.conn_dict[conn]['connection_index']==connection_index and self.conn_dict[conn]['ts']<raw_flow['ts']:
                            conn_logs.append(self.conn_dict[conn])
                        elif self.conn_dict[conn]['ts']>=raw_flow['ts']:
                            break
                valid_x509_list=self.split_ssl(ssl_log) #get first x509 from ssl record
                self.write_ssl_x509_log(ssl_log,valid_x509_list, str_connection_index)                
            elif not connection_index in self.connection_4_tuples:
                return None
            print('Connection index result: '+str(connection_index))
            conn_logs.append(conn_log)
            self.write_conn_log(conn_logs, str_connection_index)
            result_data=self.process_features(connection_index)
            return result_data

        except Exception as inst:
            # Stop the timer
            self.print('Error in process_flow()')
            self.print(traceback.print_exc(),0,1)

    def detect(self, connection_feature):
        """
        Detect this flow with the current model stored
        """
        try:
            # X_flow=self.data[connection_index]
            # X_flow = pd.DataFrame.from_dict(X_flow)
            # self.sio.emit('logs', { 'name':'detect'})
            pred = self.clf.predict(connection_feature.drop('connection_index', axis=1))
            return pred
        except Exception as inst:
            # Stop the timer
            self.print('Error in detect() X_flow:')
            self.print(connection_feature)
            self.print(traceback.print_exc(),0,1)

### CRUD MODEL

    def read_model(self):
        """
        Read the trained model from disk
        """
        try:
            self.print(f'Reading the trained model from disk.', 0, 2)
            self.clf = xgb.XGBClassifier()
            self.clf.load_model("./modules/flowmldetection/model_xgb.bin")
            # self.clf.load_model("./modules/flowmldetection/model_xgb_test.bin")
        except FileNotFoundError:
            # If there is no model, create one empty
            self.print('There was no model. Creating a new empty model.', 0, 2)
            # self.clf = SGDClassifier(
            #     warm_start=True, loss='hinge', penalty='l1'
            # )
        except EOFError:
            self.print(
                'Error reading model from disk. Creating a new empty model.',
                0,
                2,
            )
            # self.clf = SGDClassifier(
            #     warm_start=True, loss='hinge', penalty='l1'
            # )

    # def set_evidence_malicious_flow(
    #     self, saddr, sport, daddr, dport, profileid, twid, uid
    # ):
    #     """
    #     Set the evidence that a flow was detected as malicious
    #     """
    #     confidence = 0.1
    #     threat_level = 'low'
    #     attacker_direction = 'flow'
    #     category = 'Anomaly.Traffic'
    #     attacker = (
    #         str(saddr) + ':' + str(sport) + '-' + str(daddr) + ':' + str(dport)
    #     )
    #     evidence_type = 'MaliciousFlow'
    #     ip_identification = __database__.getIPIdentification(daddr)
    #     description = f'Malicious flow by ML. Src IP {saddr}:{sport} to {daddr}:{dport} {ip_identification}'
    #     timestamp = utils.convert_format(datetime.datetime.now(), utils.alerts_format)
    #     __database__.setEvidence(evidence_type, attacker_direction, attacker, threat_level, confidence, description,
    #                              timestamp, category, profileid=profileid, twid=twid)

    def shutdown_gracefully(self):
        # Confirm that the module is done processing
        __database__.publish('finished_modules', self.name)

    def run(self):
        # utils.drop_root_privs()
        r = redis.Redis(host='localhost', port=6380, decode_responses=True, db=0)
        if self.mode=='train':
            r.publish(
                channel='log_train',
                message=json.dumps({'msg':'Start extract file log','time':datetime.datetime.now().strftime('%H:%M:%S')})
                # message=json.dumps({'msg':'Start extract file log','time':str(datetime.datetime.now().time())})
            )
            # r_train_conn = redis.Redis(host='localhost', port=6380, decode_responses=True, db=1)
            # r_train_x509 = redis.Redis(host='localhost', port=6381, decode_responses=True, db=2)
            # r_train_4tuple = redis.Redis(host='localhost', port=6381, decode_responses=True, db=3)
        # Load the model
        self.read_model()
        # if self.mode=='test':
        #     while True:
        #         if os.path.isdir(self.zeek_folder) and os.path.exists(os.path.join(self.zeek_folder,'conn.log')):
        #             print('exist')
        #             break
        # #start multithreading for read conn.log, ssl.log, x509.log
        #     read_conn_log = threading.Thread(target=self.load_conn_log, name='Loading file conn.log')
        #     read_ssl_log = threading.Thread(target=self.load_ssl_log, name='Loading file ssl.log')
        #     read_x509_log = threading.Thread(target=self.load_x509_log, name='Loading file x509.log')
        #     read_conn_log.start()
        #     read_ssl_log.start()
        #     read_x509_log.start()
        while True:
            try:
                message = __database__.get_message(self.c1)

                if message and message['data'] == 'stop_process':
                    self.shutdown_gracefully()
                    return True
                if utils.is_msg_intended_for(message, 'new_flow'):
                    if self.mode == 'train':
                        conn_size=0
                        while True:
                            file1 = os.stat(os.path.join(self.zeek_folder,'conn.log')) # initial file size
                            file1_size = file1.st_size
                            # your script here that collects and writes data (increase file size)
                            comp = file1_size - conn_size # compares sizes
                            if comp == 0:
                                break
                            else:
                                conn_size=file1_size
                                time.sleep(0.2)
                        # dataset= self.process_flows(r,r_train_conn)
                        print('done extract')
                        dataset= self.process_flows(r)
                        # Train an algorithm
                        self.trainModel(dataset,r)
                        print('done train')
                        os.system("killall -9 python3")
                        os.kill(os.getppid(), signal.SIGTERM)
                    else:
                        attrs = vars(self)
                        data = message['data']
                        # Convert from json to dict
                        data = json.loads(data)
                        profileid = data['profileid']
                        twid = data['twid']
                        # Get flow that is now in json format
                        flow = data['flow']
                        # Convert flow to a dict
                        flow = json.loads(flow)
                        uid = next(iter(flow))
                        self.flow_dict = json.loads(flow[uid])
                        self.flow_dict['uid']=uid
                        # We are testing, which means using the model to detect
                        connection_feature=self.process_flow()
                        if connection_feature is not None:
                            pred=self.detect(connection_feature)
                            print('Result: '+str(pred))
                            __database__.subscribe('new_flow')
                            pub = r.publish(
                                channel='log_event',
                                message=json.dumps({ 'connection_data': connection_feature.to_dict(orient='records')[0], 'predict':str(pred[0]), 'time':self.flow_dict['ts'], 'uid':self.flow_dict['uid'],'folder':self.output_dir})
                            )
                            # r.set(self.flow_dict['uid'],json.dumps({ 'connection_data': connection_feature.to_dict(orient='records')[0], 'predict':str(pred[0]), 'time':self.flow_dict['ts']}))
                            if pred[0]==1:
                                #Generate an alert
                                # self.set_evidence_malicious_flow(
                                #     self.flow_dict['saddr'],
                                #     self.flow_dict['sport'],
                                #     self.flow_dict['daddr'],
                                #     self.flow_dict['dport'],
                                #     profileid,
                                #     twid,
                                #     uid,
                                # )
                                self.print(
                                    f'Prediction {pred[0]} for label Malware flow {self.flow_dict["saddr"]}:'
                                    f'{self.flow_dict["sport"]} -> {self.flow_dict["daddr"]}:'
                                    f'{self.flow_dict["dport"]}/{self.flow_dict["proto"]}',
                                    0,
                                    2,
                                )
                            

                        # After processing the flow, it may happen that we delete icmp/arp/etc
                        # so the dataframe can be empty
                        # if not self.flow.empty:
                        #     # Predict
                        #     pred = self.detect()
                        #     label = self.flow_dict['label']

                        #     # Report
                        #     if (
                        #         label
                        #         and label != 'unknown'
                        #         and label != pred[0]
                        #     ):
                        #         # If the user specified a label in test mode, and the label
                        #         # is diff from the prediction, print in debug mode
                        #         self.print(
                        #             f'Report Prediction {pred[0]} for label {label} flow {self.flow_dict["saddr"]}:'
                        #             f'{self.flow_dict["sport"]} -> {self.flow_dict["daddr"]}:'
                        #             f'{self.flow_dict["dport"]}/{self.flow_dict["proto"]}',
                        #             0,
                        #             3,
                        #         )
                        #     if pred[0] == 'Malware':
                        #         # Generate an alert
                        #         self.set_evidence_malicious_flow(
                        #             self.flow_dict['saddr'],
                        #             self.flow_dict['sport'],
                        #             self.flow_dict['daddr'],
                        #             self.flow_dict['dport'],
                        #             profileid,
                        #             twid,
                        #             uid,
                        #         )
                        #         self.print(
                        #             f'Prediction {pred[0]} for label {label} flow {self.flow_dict["saddr"]}:'
                        #             f'{self.flow_dict["sport"]} -> {self.flow_dict["daddr"]}:'
                        #             f'{self.flow_dict["dport"]}/{self.flow_dict["proto"]}',
                        #             0,
                        #             2,
                        #         )

            except KeyboardInterrupt:
                self.shutdown_gracefully()
                return True
            except Exception as inst:
                self.print('Error in run()')
                self.print(traceback.format_exc(), 0, 1)
                return True
