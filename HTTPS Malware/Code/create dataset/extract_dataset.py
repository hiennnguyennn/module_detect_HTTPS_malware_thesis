from connection_4_tuple import Connection4tuple
import os
from datetime import datetime
import pandas as pd
from glob import glob
import csv
from pathlib import Path
import pandas as pd
import sys
class EvaluateData():
  space='\t'
  def __init__(self):
    #self.name_of_result=name_of_result
    self.connection_4_tuples=dict() #connection 4-tuple in dataset
    self.x509_dict=dict() #list x509 record in dataset
    self.conn_dict=dict()#list connection record in dataset
    self.all_conn_dict=dict()

    self.control_ssl_uids_dict=dict() #list uids in all connection 4-tuples
    self.not_added_x509=0 #aggreation not have x509
    self.is_computed_433feature=False
    self.number_ssl_logs=0 
    self.number_x509_logs=0

  def evaluate_features(self, path_to_dataset):
    self.load_x509_file(path_to_dataset)
    self.load_conn_file(path_to_dataset)
    self.create_4_tuples(path_to_dataset)
    self.add_not_ssl_logs(path_to_dataset)
    return self.save_features(path_to_dataset)

#
  def create_4_tuples(self, path_to_dataset): #create 4-tuple for 1 dataset
    print('Createing connection 4-tuple')
    background_flows=0
    number_of_adding_x509=0
    number_adding_ssl=0 #number aggreation in dataset 
    count_lines=0
    conn_split=[] 

    self.not_added_x509=0
    #open each ssl file in dataset
    with open(path_to_dataset+"/bro/ssl.log") as ssl_file:
      for ssl_line in ssl_file:
        if '#' == ssl_line[0]:
          continue
        # print(11111)
        count_lines+=1
        ssl_split=ssl_line.split('	')
        # print(222, ssl_split)
        ssl_uid=ssl_split[1]
        # if not any(ext in ssl_split[6].lower() for ext in ['ssl','tls']):
        #   continue
        try:
          if self.control_ssl_uids_dict[ssl_uid]:
            continue
        except:
          self.control_ssl_uids_dict[ssl_uid]=1
        try:
          conn_log=self.conn_dict[ssl_uid]
          conn_split=conn_log.split('	')
        except:
          continue
        connection_index=conn_split[2], conn_split[4], conn_split[5], conn_split[6]
        try:
          label=conn_split[21]
        except IndexError:
          print("Error: no label in conn line")
        label=label.lower()
        if 'background' in label or 'No_Label' in label:
          background_flows+=1
          print('Error: background flows')
          continue
        
        if not ('botnet' in label) and not('normal' in label):
          print("Error: more state of label")
          continue

        #add conn_log to conenction 4-tuple
        try: 
          #add conn record to exist Connection 4-tuple
          self.connection_4_tuples[connection_index].add_ssl_flow(conn_log, label) #caculate connection feature
        except:
          #create new Connection 4-tuple, add conn record to Connection 4-tuple
          self.connection_4_tuples[connection_index]=Connection4tuple(connection_index)
          self.connection_4_tuples[connection_index].add_ssl_flow(conn_log, label)
        #add x509 record to SSL aggregation 
        valid_x509_list=self.split_ssl(ssl_line, connection_index, label) #get first x059 from ssl record
        number_of_adding_x509+=len(valid_x509_list)
        self.connection_4_tuples[connection_index].add_ssl_log(ssl_line, valid_x509_list, os.path.basename(path_to_dataset))
        number_adding_ssl+=1 
        self.number_ssl_logs+=1
        self.number_x509_logs+=len(valid_x509_list)
      ssl_file.close()
    self.conn_dict=dict()
    self.x509_dict=dict()
    self.control_ssl_uids_dict=dict()
    self.count_statistic_of_conn(count_lines, background_flows, number_adding_ssl, number_of_adding_x509)

  def split_ssl(self, ssl_line, tuple_index, label):
    split=ssl_line.split('	')

    if '-'==split[14] or '(object)'==split[14]:
      self.not_added_x509+=1
      return []
    #self.put_server_name_to_dict(split[1], split[9], tuple_index, split[14], label)
    return [self.get_x509_line(split[14].split(','))]
  #def put_server_name_to_dict(self, ssl_uid, server):
  def get_x509_line(self, x509_uids_list):
    x509_line=None
    uid_x509=x509_uids_list[0]
    try:
      if self.x509_dict[uid_x509]:
        x509_line=self.x509_dict[uid_x509][0]
        if len(self.x509_dict[uid_x509])>1:
          print("Error:Error: [ProcessLogs] Actual ssl flow needs x509 log, which has more same uids!!!!")
    except:
      print("Error: [get_x509_lines] In ProcessLogs.py x509 does not have this x509uid: "+x509_uids_list[0])
    return x509_line

  def count_statistic_of_conn(self, number_of_lines, background_flows, number_adding_ssl, number_adding_x509):
    malware_tuple=0
    normal_tuple=0
    malware_flows=0
    normal_flows=0
    for key in self.connection_4_tuples.keys():
      if int(len(self.connection_4_tuples[key].uid_flow_dict))!=int(self.connection_4_tuples[key].number_of_connection_records()):
        print("Error: connection uid and number of connection not same")
      if self.connection_4_tuples[key].is_malware():
        malware_tuple+=1
        malware_flows += self.connection_4_tuples[key].number_of_connection_records()
      else:
        normal_tuple+=1
        normal_flows += self.connection_4_tuples[key].number_of_connection_records()
    #print(malware_tuple, normal_tuple)

#loading, add all connection record to conn_dict
  def load_conn_file(self, path_to_dataset):
    print('Loading conn file')
    space='\t'
    file = open(path_to_dataset+'/bro/conn_label1.log', 'r')
    lines = file.readlines()
    for line in lines:
      if '#' ==line[0]:
          continue
      split=line.split(space)
      conn_uid=split[1]
      label=split[21]
      if 'background' in label or 'no_label' in label:
        continue
      try:
        if self.conn_dict[conn_uid]:
          print("Error: More conn log with same uid")
      except:
        self.conn_dict[conn_uid]=line

    # with open(path_to_dataset+'/bro/conn_label.log') as f:
    #   for line in f:
    #     if '#' ==line[0]:
    #       continue
    #     split=line.split(space)
    #     conn_uid=split[1]
    #     label=split[21]
    #     if 'background' in label or 'no_label' in label:
    #       continue
    #     try:
    #       if self.conn_dict[conn_uid]:
    #         print("Error: More conn log with same uid")
    #     except:
    #       self.conn_dict[conn_uid]=line
    #   f.close()

#edit datetime in x509.log
  def load_x509_file(self, path_to_dataset):
    print('Loading x509 file')
    started_unix_time=0.0
    try:
      path=path_to_dataset + '/start_date.txt'
      with open(path) as f:
        started_date = f.readline().split(',')
        utc_date=self.from_cet_to_utc(started_date)
        started_unix_time=self.from_utc_to_timestamp(utc_date)
        #print(started_unix_time)

    except:
      print('no start_date.txt')
    
    try:
      with open(path_to_dataset+'/bro/x509.log') as f:
        for line in f:
          if '#' ==line[0]:
            continue
          split=line.split('\t')
          time_new=float(split[0])+started_unix_time
          new_line=str(time_new)
          for i in range(1, len(split)):
            new_line+='\t'+split[i]
          x509_uid=split[1]
          try:
            self.x509_dict[x509_uid].append(new_line)
          except:
            self.x509_dict[x509_uid]=[]
            self.x509_dict[x509_uid].append(new_line)
        f.close()
    except IOError:
      print("no x509 file")
      
  def from_cet_to_utc(self,started_date):
    date=started_date[0].split('/')
    time=started_date[1].strip().rstrip().split(':')
    date='-'.join(map(str, date))
    time=':'.join(map(str, time))
    df = pd.DataFrame({'date': pd.to_datetime([date+' '+time])})
    df=df['date'].dt.tz_localize('Europe/Brussels').dt.tz_convert('UTC').dt.strftime('%Y-%m-%d %H:%M:%S')
    return df[0]
  def from_utc_to_timestamp(self,date):
    datetime_object = datetime.strptime(date, '%Y-%m-%d %H:%M:%S')
    return datetime_object.timestamp()

#add conn record not in aggregation to Connection 4-tuple
  def add_not_ssl_logs(self, path_to_dataset):
    print("adding conn record not belong to aggregation")
    not_ssl_conn=0
    space='\t'
    ssl_conn=0
    tmp=0
    
    with open(path_to_dataset+'/bro/conn_label1.log') as f:
      for line in f:
        if '#'==line[0]:
          continue
        conn_split=line.split(space)
        connection_index=conn_split[2], conn_split[4], conn_split[5], conn_split[6]
        try:
          label=conn_split[21]
        except IndexError:
          continue
        conn_uid=conn_split[1]
        if 'Background' in label or 'No_Label' in label:
          continue
        if connection_index in self.connection_4_tuples:
          tmp+=1
          if conn_uid in self.connection_4_tuples[connection_index].uid_flow_dict:
            ssl_conn+=1
          else:
            self.connection_4_tuples[connection_index].add_not_ssl_flow(line, label)
            not_ssl_conn += 1
        else:
          continue
          #print(conn_uid)

    f.close()
    print("          <<< not ssl conn:"+str(not_ssl_conn))
    print("          <<< ssl conn:"+str(ssl_conn))
              
  def save_features(self, path_to_dataset):
    print('Saving features')
    data=[]
    headers=[]
    for key in self.connection_4_tuples.keys():      
      d=dict()
      d['connection_index']=key
      d['dataset']=path_to_dataset.split('/')[-1]
      d['number_of_connection_records']=self.connection_4_tuples[key].number_of_connection_records()
      d['mean_of_duration']=self.connection_4_tuples[key].mean_of_duration()
      d['standard_deviation_of_duration']=self.connection_4_tuples[key].standard_deviation_of_duration()
      d['standard_deviation_range_duration']=self.connection_4_tuples[key].standard_deviation_range_duration()
      d['payload_bytes_from_originator']=self.connection_4_tuples[key].payload_bytes_from_originator()
      d['payload_bytes_from_responder']=self.connection_4_tuples[key].payload_bytes_from_responder()
      # d['mean_payload_bytes_from_responder']=self.connection_4_tuples[key].mean_payload_bytes_from_responder()
      # d['standard_payload_bytes_from_responder']=self.connection_4_tuples[key].standard_payload_bytes_from_responder()
      d['ratio_of_responder_bytes']=self.connection_4_tuples[key].ratio_of_responder_bytes()
      # d['ratio_established_states']=self.connection_4_tuples[key].ratio_established_states()
      d['inbound_packets']=self.connection_4_tuples[key].get_inbound_packets()
      d['outbound_packets']=self.connection_4_tuples[key].get_outbound_packets()
      d['periodicity_mean']=self.connection_4_tuples[key].periodicity_mean()
      d['standart_deviation_of_periodicity']=self.connection_4_tuples[key].standart_deviation_of_periodicity()
      d['ratio_connection_record_ssl_aggregations']=self.connection_4_tuples[key].ratio_connection_record_ssl_aggregations()
      d['ratio_tls_ssl']=self.connection_4_tuples[key].ratio_tls_ssl()
      d['ratio_sni']=self.connection_4_tuples[key].ratio_sni()
      # d['sni_as_ip']=self.connection_4_tuples[key].sni_as_ip()
      d['mean_certificate_path']=self.connection_4_tuples[key].mean_certificate_path()
      # d['ratio_self_signed_certificate']=self.connection_4_tuples[key].ratio_self_signed_certificate()
      d['public_key_mean']=self.connection_4_tuples[key].public_key_mean()
      d['mean_certificate_validatity_periods']=self.connection_4_tuples[key].mean_certificate_validatity_periods()
      d['standard_deviation_certificate_validatity_periods']=self.connection_4_tuples[key].standard_deviation_certificate_validatity_periods()
      # d['validity_certificate_period_during_capturing']=self.connection_4_tuples[key].validity_certificate_period_during_capturing()
      d['mean_age_of_cert']=self.connection_4_tuples[key].mean_age_of_cert()
      d['amount_of_certificates']=self.connection_4_tuples[key].amount_of_certificates()
      d['mean_number_of_domains_in_SAN_DNS']=self.connection_4_tuples[key].mean_number_of_domains_in_SAN_DNS()
      d['ratio_certificate_records_SSL_records']=self.connection_4_tuples[key].ratio_certificate_records_SSL_records()
      d['sni_in_san_dns']=self.connection_4_tuples[key].sni_in_san_dns()
      d['cn_in_san_dns']=self.connection_4_tuples[key].cn_in_san_dns()
      d['label']=self.connection_4_tuples[key].label_connection_number()
      data.append(d)
      #headers=list(d.keys())
    return data
    
def main(folder,sub_folders, file_path):
  dataset=pd.DataFrame()
  for index in range(0, len(sub_folders),1):
      print("Reading folder "+sub_folders[index])
      sub_folder_name=os.path.join(folder,sub_folders[index])
      process_log=EvaluateData()
      data=process_log.evaluate_features(sub_folder_name)
      
      dataset = pd.concat([dataset, pd.DataFrame(data)], ignore_index=True)
  dataset.to_csv(file_path, mode='a', header=True, index=False)
main('/home/hiennnn/Documents/DATN/normal_traffic',['traffic 7','traffic 8', 'traffic 9', 'traffic 10'],'/home/hiennnn/Documents/DATN/normal_traffic/testset_scenario2.csv')