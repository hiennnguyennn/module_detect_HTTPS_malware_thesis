import sys
from bs4 import BeautifulSoup
import requests
from urllib.request import urlopen
import shutil
import pandas as pd
import os
from pathlib import Path
from glob import glob
import wget

url = "https://mcfp.felk.cvut.cz/publicDatasets/"
def find_files(url):  
    soup = BeautifulSoup(requests.get(url).text, "lxml") #truy xuat thanh phan file xml
    hrefs = []
    for a in soup.find_all('a'): #get tat ca <a> trong xml
        try:
            hrefs.append(a['href'])
        except:
            pass
    return hrefs
# list_folder=glob("/home/hiennn/Documents/DATN/HTTPS malware/Detection HTTPS malware traffic - Strasak-Frantisek/Code/download_datasets/*/", recursive = True)
def download_bro_file(list_folder):
    for i in range (0,len(list_folder)):
        list_folder[i]=str(list_folder[i])
        dataset_name=list_folder[i]
        folder=Path(__file__).parents[1]

        folder = folder / 'download_datasets'
        directiry_name = str(folder)+'/' + dataset_name
        os.makedirs(directiry_name) #tao folder luu dataset/dataset_name
        folder_path = directiry_name +"/bro"
        os.makedirs(folder_path) #tao folder luu dataset/dataset_name/bro
    
        bro = find_files(url + dataset_name + '/bro/')
        for j in range(len(bro)):
            bro[j]=str(bro[j])    
            if 'conn.log' in bro[j] or 'ssl.log' in bro[j] or 'x509.log' in bro[j]:
                try:
                    print(bro[j], "is downloading...")
                    file_url=url+dataset_name+"/bro/"+bro[j]
                    file_name=folder_path+"/"+bro[j]
                    wget.download(url=""+file_url,out=""+file_name)
                    #!wget -O $file_name $file_url
                except:
                    print("Error:", bro[j], "is not able to downloaded.")

def download_pcap_file(list_folder):
    dataset=pd.read_csv('/home/hiennn/Documents/HTTPS malware/Detection HTTPS malware traffic - Strasak-Frantisek/Code/extract_dataset1.csv')
    value=dataset['dataset'].unique()
    for i in range (0,len(list_folder)):
    #for name in value:
        name=value[i]
        folder=Path(__file__).parents[1]

        folder = folder / 'download_datasets'
        directiry_name = str(folder)+'/' + name
        if os.path.exists(directiry_name):
            continue
        os.makedirs(directiry_name)
        folder_path = directiry_name
        sub = find_files(url + name)
        for j in range(len(sub)):
                sub[j]=str(sub[j])    
                if 'pcap' in sub[j].split('.')[-1]:
                    try:
                        print(sub[j], "is downloading...")
                        file_url=url+name+'/'+sub[j]
                        file_name=folder_path+"/"+'_'.join(sub[j].split('.')[:-1])+"."+sub[j].split('.')[-1]
                        wget.download(file_url, out=file_name)
                        #!wget -O $file_name $file_url
                    except Exception as err:
                        print("Error:", sub[j], "is not able to downloaded.")