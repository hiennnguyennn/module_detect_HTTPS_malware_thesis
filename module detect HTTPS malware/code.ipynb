{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "import numpy\n",
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import wget\n",
    "from datetime import datetime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conn.log is downloading...\n",
      "ssl.log is downloading...\n",
      "x509.log is downloading...\n"
     ]
    }
   ],
   "source": [
    "url = \"https://mcfp.felk.cvut.cz/publicDatasets/\"\n",
    "# get list all file in url\n",
    "def find_files(url):  \n",
    "    soup = BeautifulSoup(requests.get(url).text, \"lxml\") #truy xuat thanh phan file xml\n",
    "    hrefs = []\n",
    "    for a in soup.find_all('a'): #get tat ca <a> trong xml\n",
    "        try:\n",
    "            hrefs.append(a['href'])\n",
    "        except:\n",
    "            pass\n",
    "    return hrefs\n",
    "\n",
    "def download_bro_file(list_folder):\n",
    "    for i in range (0,len(list_folder)):\n",
    "        list_folder[i]=str(list_folder[i])\n",
    "        dataset_name=list_folder[i]\n",
    "        folder = os.path.join(os.getcwd(),'download_datasets')\n",
    "        directiry_name = str(folder)+'/' + dataset_name\n",
    "        os.makedirs(directiry_name) #tao folder luu dataset/dataset_name\n",
    "        folder_path = directiry_name +\"/bro\"\n",
    "        os.makedirs(folder_path) #tao folder luu dataset/dataset_name/bro\n",
    "    \n",
    "        bro = find_files(url + dataset_name + '/bro/')\n",
    "        for j in range(len(bro)):\n",
    "            bro[j]=str(bro[j])    \n",
    "            if 'conn.log' in bro[j] or 'ssl.log' in bro[j] or 'x509.log' in bro[j]:\n",
    "                try:\n",
    "                    print(bro[j], \"is downloading...\")\n",
    "                    file_url=url+dataset_name+\"/bro/\"+bro[j]\n",
    "                    file_name=folder_path+\"/\"+bro[j]\n",
    "                    wget.download(url=\"\"+file_url,out=\"\"+file_name)\n",
    "                    #!wget -O $file_name $file_url\n",
    "                except:\n",
    "                    print(\"Error:\", bro[j], \"is not able to downloaded.\")\n",
    "list_folder=['CTU-Malware-Capture-Botnet-54', 'CTU-Malware-Capture-Botnet-157-1', 'CTU-Normal-27', 'CTU-Malware-Capture-Botnet-25-2', 'CTU-Malware-Capture-Botnet-116-1', 'CTU-Malware-Capture-Botnet-188-4', 'CTU-Malware-Capture-Botnet-163-1', 'CTU-Malware-Capture-Botnet-50', 'CTU-Malware-Capture-Botnet-112-4', 'CTU-Malware-Capture-Botnet-120-1', 'CTU-Malware-Capture-Botnet-1', 'CTU-Malware-Capture-Botnet-112-1', 'CTU-Malware-Capture-Botnet-52', 'CTU-Malware-Capture-Botnet-142-1', 'CTU-Normal-6-filterd', 'CTU-Normal-9', 'CTU-Malware-Capture-Botnet-69', 'CTU-Malware-Capture-Botnet-137-1', 'CTU-Malware-Capture-Botnet-17-1', 'CTU-Normal-33', 'CTU-Malware-Capture-Botnet-31-1', 'CTU-Malware-Capture-Botnet-51', 'CTU-Malware-Capture-Botnet-25-1', 'CTU-Normal-14', 'CTU-Malware-Capture-Botnet-141-2', 'CTU-Malware-Capture-Botnet-138-1', 'CTU-Malware-Capture-Botnet-35-1', 'CTU-Malware-Capture-Botnet-43', 'CTU-Malware-Capture-Botnet-153-1', 'CTU-Normal-12', 'CTU-Malware-Capture-Botnet-25-4', 'CTU-Normal-7', 'CTU-Malware-Capture-Botnet-123-1', 'CTU-Malware-Capture-Botnet-162-2', 'CTU-Malware-Capture-Botnet-47', 'CTU-Malware-Capture-Botnet-140-2', 'CTU-Malware-Capture-Botnet-44', 'CTU-Malware-Capture-Botnet-110-1', 'CTU-Normal-32', 'CTU-Malware-Capture-Botnet-162-1', 'CTU-Malware-Capture-Botnet-129-1', 'CTU-Malware-Capture-Botnet-169-3', 'CTU-Malware-Capture-Botnet-102', 'CTU-Malware-Capture-Botnet-53', 'CTU-Malware-Capture-Botnet-45', 'CTU-Normal-8-1', 'CTU-Malware-Capture-Botnet-116-2', 'CTU-Malware-Capture-Botnet-208-2', 'CTU-Malware-Capture-Botnet-111-5', 'CTU-Malware-Capture-Botnet-188-2', 'CTU-Malware-Capture-Botnet-42', 'CTU-Malware-Capture-Botnet-48', 'CTU-Normal-25', 'CTU-Malware-Capture-Botnet-49', 'CTU-Malware-Capture-Botnet-25-3', 'CTU-Malware-Capture-Botnet-112-2', 'CTU-Normal-31', 'CTU-Malware-Capture-Botnet-25-5', 'CTU-Malware-Capture-Botnet-145-1', 'CTU-Malware-Capture-Botnet-111-1', 'CTU-Malware-Capture-Botnet-143-1', 'CTU-Malware-Capture-Botnet-25-6', 'CTU-Malware-Capture-Botnet-90', 'CTU-Normal-30', 'CTU-Malware-Capture-Botnet-27-1', 'CTU-Malware-Capture-Botnet-144-1', 'CTU-Malware-Capture-Botnet-17-2', 'CTU-Normal-23', 'CTU-Malware-Capture-Botnet-141-1', 'CTU-Normal-24', 'CTU-Malware-Capture-Botnet-61-1', 'CTU-Malware-Capture-Botnet-27-2', 'CTU-Malware-Capture-Botnet-219-2', 'CTU-Malware-Capture-Botnet-116-4', 'CTU-Malware-Capture-Botnet-46', 'CTU-Malware-Capture-Botnet-140-1', 'CTU-Malware-Capture-Botnet-78-1', 'CTU-Malware-Capture-Botnet-110-2', 'CTU-Malware-Capture-Botnet-78-2']\n",
    "download_bro_file(list_folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection 4-tuple class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Connection4tuple:\n",
    "  def __init__(self, tuple_index):\n",
    "    self.tuple_index=tuple_index\n",
    "\n",
    "  #list flows\n",
    "    self.ssl_flow_list=[]\n",
    "    self.not_ssl_flow_list=[]\n",
    "    self.x509_list=[]\n",
    "    self.ssl_logs_list=[]\n",
    "    self.malware_label=0\n",
    "    self.normal_label=0\n",
    "    self.datasets_names_list=[]\n",
    "\n",
    "  #connection features\n",
    "    self.uid_flow_dict=dict()\n",
    "    self.state_of_connection_dict=dict()\n",
    "    self.total_size_of_flows_orig=0\n",
    "    self.total_size_of_flows_resp=0\n",
    "    self.flow_which_has_duration_number=0\n",
    "    self.duration_list=[]\n",
    "    self.list_payload_bytes_from_resp=[]\n",
    "    self.average_duration=0\n",
    "    self.agverage_duration_power=0\n",
    "    self.inbound_packets=0\n",
    "    self.outbound_packets=0\n",
    "\n",
    "\n",
    "  #ssl features\n",
    "    self.ssl_uids_list=[]\n",
    "    self.version_of_ssl_dict=dict()\n",
    "    self.version_of_ssl_cipher_dict=dict()\n",
    "    self.certificate_path=dict()\n",
    "    self.ssl_with_SNI=0\n",
    "    self.SNI_list=[]\n",
    "    self.SNI_equal_DstIP=1\n",
    "    self.self_signed_cert=0\n",
    "\n",
    "  #x509 features\n",
    "    self.is_CN_in_SAN_list=[]\n",
    "    self.is_SNI_in_san_dns=[]\n",
    "    self.not_valid_certificate_number=0\n",
    "    self.cert_percent_validity=[]\n",
    "    self.certificate_serial_dict=dict()\n",
    "    self.certificate_key_length_dict=dict()\n",
    "    self.temp_list=[]\n",
    "    self.certificate_valid_length=0\n",
    "    self.certificate_valid_length_pow=0\n",
    "    self.certificate_valid_number=0\n",
    "    self.number_san_domains=0\n",
    "    self.number_san_domains_index=0\n",
    "\n",
    "    self.data=dict()\n",
    "\n",
    "# connection main\n",
    "  #ssl log is ssl record\n",
    "  #ssl_flow=has ssl log in ssl file (conn record associate with any ssl reocrd)\n",
    "  def add_ssl_flow(self, flow, label):\n",
    "    if 'botnet' in label:\n",
    "      self.malware_label +=1\n",
    "    elif 'normal' in label:\n",
    "      self.normal_label +=1\n",
    "    else:\n",
    "      print(\"Error in Connection 4-tuple: Here is label which not normal or malware, it is \"+label)\n",
    "    self.ssl_flow_list.append(flow)\n",
    "    self.compute_classic_features(flow)\n",
    "\n",
    "  #ssl_flow=does not have ssl log in ssl file (conn record not associate with any ssl record)\n",
    "  def add_not_ssl_flow(self, flow, label):\n",
    "    if 'botnet' in label:\n",
    "      self.malware_label +=1\n",
    "    elif 'normal' in label:\n",
    "      self.normal_label +=1\n",
    "    else:\n",
    "      print(\"Error in Connection 4-tuple: Here is label which not normal or malware, it is \"+label)\n",
    "    self.not_ssl_flow_list.append(flow)\n",
    "    self.compute_classic_features(flow)\n",
    "\n",
    "# ssl and x509 main\n",
    "  def add_ssl_log(self, ssl_log, valid_x509_list, dataset_name): #ssl_log: a ssl line, valid_x509_list: list x509 record related with this ssl log\n",
    "    for x509_record in valid_x509_list:\n",
    "      self.compute_x509_features(x509_record)\n",
    "      self.is_SNI_in_SAN(ssl_log, x509_record)\n",
    "    self.compute_ssl_features(ssl_log)\n",
    "    if not(dataset_name in self.datasets_names_list):\n",
    "      self.datasets_names_list.append(dataset_name)\n",
    "  \n",
    "    \n",
    "#computing connection feature method\n",
    "#uid: unique id of connection, Bro logs có sự liên kết giữa chúng với nhau thông qua các khóa duy nhất (unique key), mỗi dòng trong bất kỳ logs nào cũng đều có unique key liên kết đến các dòng khác trong phần còn lại của logs. \n",
    "  def compute_classic_features(self, flow):\n",
    "    split=flow.split('\t') #split feature in flow\n",
    "    #count each uid in connection 4-tuple\n",
    "    try:\n",
    "      self.uid_flow_dict[split[1]]+=1\n",
    "      print(\"Error: More same conn uids in compute_classic_features function\")\n",
    "    except:\n",
    "      self.uid_flow_dict[split[1]]=1\n",
    "    self.add_state_of_connection(split[11]) #split[11]: conn_state\n",
    "    self.compute_size_of_flow(split[9], split[10]) #split[10]:resp-bytes (Responder payload bytes; from sequence numbers if TCP), split[9]: orig_bytes (Originator payload bytes; from sequence numbers if TCP)\n",
    "    try:\n",
    "      duration=float(split[8]) #split[8]: duration (Time of last packet seen – time of first packet seen)\n",
    "      self.process_duration(duration)\n",
    "    except:\n",
    "      pass\n",
    "    try:\n",
    "      self.inbound_packets += int(split[18]) #split[18]: resp_pkts (Number of RESP packets)\n",
    "    except:\n",
    "      print(\"Error: resp pckts has bad formats\")\n",
    "    try:\n",
    "      self.outbound_packets += int(split[16]) #split[16]: orig_pkts (Number of ORIG packets)\n",
    "    except:\n",
    "      print(\"Error: orig pckts has bad formats\")\n",
    "    \n",
    "\n",
    "\n",
    "  #count each state of connection in connection 4-tuple\n",
    "  def add_state_of_connection(self, state):\n",
    "    if state not in self.state_of_connection_dict.keys():\n",
    "      self.state_of_connection_dict[state]=1\n",
    "    else:\n",
    "      self.state_of_connection_dict[state]+=1\n",
    "\n",
    "\n",
    "  def process_duration(self, duration_value):\n",
    "    self.flow_which_has_duration_number+=1\n",
    "    self.duration_list.append(duration_value)\n",
    "    self.average_duration+=duration_value #EX of duration\n",
    "    self.agverage_duration_power+=pow(duration_value,2) # EX^2 of \n",
    "  def compute_size_of_flow(self, orig_bytes, resp_bytes):\n",
    "    try:\n",
    "      orig_bytes_number=int(orig_bytes)\n",
    "    except:\n",
    "      if orig_bytes!='-':\n",
    "        print(\"Error: orig_bytes has bad format\")\n",
    "      orig_bytes_number=0\n",
    "    try:\n",
    "      resp_bytes_number=int(resp_bytes)\n",
    "    except:\n",
    "      if resp_bytes!='-':\n",
    "        print(\"Error: resp_bytes has bad format\")\n",
    "      resp_bytes_number=0\n",
    "    self.list_payload_bytes_from_resp.append(resp_bytes_number)\n",
    "    self.total_size_of_flows_orig+=orig_bytes_number\n",
    "    self.total_size_of_flows_resp+=resp_bytes_number\n",
    "\n",
    "#computing ssl features\n",
    "  def compute_ssl_features(self, ssl_log):\n",
    "    self.ssl_logs_list.append(ssl_log)\n",
    "    split=ssl_log.split('\t')\n",
    "    self.ssl_uids_list.append(split[1]) #split[1]: uid\n",
    "\n",
    "    try:\n",
    "      self.version_of_ssl_dict[split[6].lower()]+=1 #split[6]:version, SSL version that the server offered\n",
    "    except:\n",
    "      self.version_of_ssl_dict[split[6].lower()]=1\n",
    "\n",
    "    try:\n",
    "      self.version_of_ssl_cipher_dict[split[7]]+=1#split[6]:cipher, SSL cipher suite that the server choose\n",
    "    except:\n",
    "      self.version_of_ssl_cipher_dict[split[7]]=1\n",
    "    if split[14]!='-': #split[14]: cert_chain_fuids, certificate path\n",
    "      list_of_x509_uids=split[14].split(',')\n",
    "      try:\n",
    "        self.certificate_path[len(list_of_x509_uids)]+=1\n",
    "      except:\n",
    "        self.certificate_path[len(list_of_x509_uids)]=1\n",
    "    server_name=split[9]\n",
    "    if server_name!='-':\n",
    "      self.ssl_with_SNI+=1\n",
    "      self.SNI_list.append(server_name)\n",
    "      if self.SNI_equal_DstIP!=-1:\n",
    "        try:\n",
    "          socket.inet_aton(server_name)\n",
    "          dstIP=self.tuple_index[1]\n",
    "          print(\"SNI as IP: \", server_name,\" and destination ip is: \", dstIP)\n",
    "          if(dstIP!=server_name):\n",
    "            self.SNI_equal_DstIP=-1\n",
    "          else:\n",
    "            self.SNI_equal_DstIP=0\n",
    "        except:\n",
    "          pass\n",
    "      try:\n",
    "        if 'signed certificate in certificate' in split[20]: #split[20]??????????\n",
    "          self.self_signed_cert+=1\n",
    "      except:\n",
    "        pass\n",
    "\n",
    "#computing x509 features\n",
    "  def compute_x509_features(self, valid_x509_line):\n",
    "    self.x509_list.append(valid_x509_line)\n",
    "    self.is_CN_in_SAN(valid_x509_line)\n",
    "    split=valid_x509_line.split('\t')\n",
    "    \n",
    "    #check if certificate valid during capture\n",
    "    if split[7]!='-' and split[6]!='-': #split[6]: certificate_not_valid_before, split[7]: certificate_not_valid_after\n",
    "      try:\n",
    "        current_time=float(split[0])\n",
    "        before_date=float(split[6])\n",
    "        after_date=float(split[7])\n",
    "        if current_time>after_date or current_time<before_date:\n",
    "          self.not_valid_certificate_number+=1\n",
    "        norm_after=after_date-before_date #time valid certificate\n",
    "        current_time_norm=current_time-before_date #time valid certificate to now\n",
    "        self.cert_percent_validity.append(current_time_norm/norm_after)\n",
    "      except:\n",
    "        print(\"Certificate time length is broken\")\n",
    "    if not (split[3] in self.certificate_serial_dict.keys()): #split[3]: certificate serial: serial number of certificate\n",
    "      self.certificate_serial_dict[split[3]]=1\n",
    "      if split[11]!='-': #split[11]: certificate key length in bits\n",
    "        try:\n",
    "          self.certificate_key_length_dict[split[11]]+=1\n",
    "        except:\n",
    "          self.certificate_key_length_dict[split[11]]=1\n",
    "      if(split[7]!='-' and split[6]!='-'):\n",
    "        try:\n",
    "          valid_length_sec=float(split[7])-float(split[6])\n",
    "          valid_length_day_not_round=int(valid_length_sec/(3600*24))\n",
    "          valid_length_day=round(valid_length_day_not_round,2)\n",
    "\n",
    "          self.temp_list.append(valid_length_day)\n",
    "          self.certificate_valid_length+=valid_length_day\n",
    "          self.certificate_valid_length_pow+=pow(valid_length_day,2)\n",
    "          self.certificate_valid_number+=1\n",
    "        except:\n",
    "          pass\n",
    "      if split[14]!='-': #split[14]: san.dns: List of DNS entries in SAN\n",
    "        domains=len(split[14].split(','))\n",
    "        self.number_san_domains+=domains\n",
    "        self.number_san_domains_index+=1\n",
    "    else:\n",
    "      self.certificate_serial_dict[split[3]]+=1\n",
    "\n",
    "\n",
    "  def get_periodicity_list(self):\n",
    "    final_flow_list = self.ssl_flow_list + self.not_ssl_flow_list\n",
    "    flows_times_list = []\n",
    "    for i in range(len(final_flow_list)):\n",
    "      split = final_flow_list[i].split('\t')\n",
    "      flows_times_list.append(float(split[0]))\n",
    "    sorted_times_list = sorted(flows_times_list)\n",
    "    T2_1 = None\n",
    "    T2_2 = None\n",
    "    T3 = None\n",
    "    last_flow = None\n",
    "    time_diff_list = []\n",
    "    for i in range(len(sorted_times_list)):\n",
    "      if last_flow == None:\n",
    "        last_flow = sorted_times_list[i]\n",
    "        continue\n",
    "      if T2_1 == None:\n",
    "        T2_1 = sorted_times_list[i] - last_flow\n",
    "        last_flow = sorted_times_list[i]\n",
    "        continue\n",
    "\n",
    "      T2_2 = sorted_times_list[i] - last_flow\n",
    "      T3 = abs(T2_2 - T2_1)\n",
    "      T2_1 = T2_2\n",
    "      last_flow = sorted_times_list[i]\n",
    "      time_diff_list.append(T3)\n",
    "    return time_diff_list\n",
    "\n",
    "        \n",
    "\n",
    "  def is_CN_in_SAN(self, x509_record):\n",
    "    split=x509_record.split('\t')\n",
    "    if split[14]!='-': #split[14]: san.dns: List of DNS entries in SAN\n",
    "      CN_part=split[4] #split[4]: CN\n",
    "      SAN_dns_list=split[14].split(',')\n",
    "      for i in range(len(SAN_dns_list)):\n",
    "        if '*' in SAN_dns_list[i]:\n",
    "          SAN_dns_list[i]=SAN_dns_list[i].replace('*', '')\n",
    "      tmp=0\n",
    "      for san_dns in SAN_dns_list:\n",
    "        if san_dns in CN_part:\n",
    "          tmp=1\n",
    "          break\n",
    "      self.is_CN_in_SAN_list.append(tmp)\n",
    "    \n",
    "  def is_SNI_in_SAN(self, ssl_record, x509_record):\n",
    "    ssl_split=ssl_record.split('\t')\n",
    "    x509_split=x509_record.split('\t')\n",
    "    server_name=ssl_split[9]\n",
    "    if server_name!='-':\n",
    "      if x509_split[14]!='-': #x509_split[14]: san dns\n",
    "        san_dns_list=x509_split[14].split(',')\n",
    "        for i in range(len(san_dns_list)):\n",
    "          if '*' in san_dns_list[i]:\n",
    "            san_dns_list[i]=san_dns_list[i].replace('*','')\n",
    "        tmp=0\n",
    "        for san_dns in san_dns_list:\n",
    "          if san_dns in server_name:\n",
    "            tmp=1\n",
    "            break\n",
    "        self.is_SNI_in_san_dns.append(tmp)\n",
    "      \n",
    "#GET FEATURES\n",
    "\n",
    "  #feature 1\n",
    "  def number_of_connection_records(self):\n",
    "    return len(self.ssl_flow_list)+len(self.not_ssl_flow_list)\n",
    "    #return self.number_+self.get_number_of_not_ssl_flows()\n",
    "  #feature 2\n",
    "  def mean_of_duration(self):\n",
    "    if self.flow_which_has_duration_number !=0:\n",
    "      return numpy.mean(self.duration_list)\n",
    "      #return self.average_duration/self.flow_which_has_duration_number\n",
    "    return -1\n",
    "  #feature 3\n",
    "  def standard_deviation_of_duration(self):\n",
    "    if len(self.duration_list)!=0:\n",
    "      return numpy.std(self.duration_list)\n",
    "    return -1\n",
    "  #feature 4\n",
    "  def standard_deviation_range_duration(self):\n",
    "    if len(self.duration_list)!=0:\n",
    "      out_of_bounds=0\n",
    "      lower_level=self.mean_of_duration()-self.standard_deviation_of_duration()\n",
    "      higher_level=self.mean_of_duration()+self.standard_deviation_of_duration()\n",
    "      for duration in self.duration_list:\n",
    "        if duration<lower_level or duration>higher_level:\n",
    "          out_of_bounds+=1\n",
    "      return out_of_bounds/self.flow_which_has_duration_number\n",
    "    return -1\n",
    "  #feature 5\n",
    "  def payload_bytes_from_originator(self):\n",
    "    return self.total_size_of_flows_orig\n",
    "  #feature 6\n",
    "  def payload_bytes_from_responder(self):\n",
    "    return self.total_size_of_flows_resp\n",
    "  def mean_payload_bytes_from_responder(self):\n",
    "    if self.list_payload_bytes_from_resp:\n",
    "      return numpy.mean(self.list_payload_bytes_from_resp)\n",
    "    return -1\n",
    "  def standard_payload_bytes_from_responder(self):\n",
    "    if self.list_payload_bytes_from_resp:\n",
    "      return numpy.std(self.list_payload_bytes_from_resp)\n",
    "    return -1\n",
    "  #feature 7\n",
    "  def ratio_of_responder_bytes(self):\n",
    "    if self.total_size_of_flows_orig+self.total_size_of_flows_resp!=0:\n",
    "      return self.total_size_of_flows_resp/(self.total_size_of_flows_orig+self.total_size_of_flows_resp)\n",
    "    return -1\n",
    "  #feature 8\n",
    "  def ratio_established_states(self):\n",
    "    established_states=0\n",
    "    total_value_states=0\n",
    "    for key in self.state_of_connection_dict.keys():\n",
    "      total_value_states+=self.state_of_connection_dict[key]\n",
    "    if total_value_states!=0:\n",
    "      established_states+=self.state_of_connection_dict.get('SF',0)\n",
    "      established_states+=self.state_of_connection_dict.get('S1',0)\n",
    "      established_states+=self.state_of_connection_dict.get('S2',0)\n",
    "      established_states+=self.state_of_connection_dict.get('S3',0)\n",
    "      established_states+=self.state_of_connection_dict.get('RSTO',0)\n",
    "      established_states+=self.state_of_connection_dict.get('RSTR',0)\n",
    "      return established_states/total_value_states\n",
    "    return -1\n",
    "  #feature 9\n",
    "  def get_inbound_packets(self):\n",
    "    return self.inbound_packets\n",
    "  #feature 10\n",
    "  def get_outbound_packets(self):\n",
    "    return self.outbound_packets\n",
    "  #feature 11\n",
    "  #def periodicity_mean(self):\n",
    "  #feature 12\n",
    "  #def standart_deviation_periodicity(self):\n",
    "  def periodicity_mean(self):\n",
    "    per_list = self.get_periodicity_list()\n",
    "    sum = 0\n",
    "    for i in range(len(per_list)):\n",
    "      sum += per_list[i]\n",
    "      if len(per_list) != 0:\n",
    "        return sum / float(len(per_list))\n",
    "        # print \"periodicity list is zero. Number of flows:\", self.get_number_of_flows()\n",
    "    return -1\n",
    "\n",
    "    # 14\n",
    "  def standart_deviation_of_periodicity(self):\n",
    "    per_list = self.get_periodicity_list()\n",
    "    if len(per_list) != 0:\n",
    "            # sum = 0\n",
    "            # for i in range(len(per_list)):\n",
    "            #     sum += pow(per_list[i], 2)\n",
    "            # EX2 = sum / float(len(per_list))\n",
    "            # DX = EX2 - EX * EX\n",
    "            # return pow(DX, 0.5)\n",
    "      return numpy.std(self.get_periodicity_list())\n",
    "    return -1\n",
    "  #feature 13\n",
    "  def ratio_connection_record_ssl_aggregations(self):\n",
    "    return len(self.not_ssl_flow_list)/len(self.ssl_flow_list)\n",
    "  #feature 14\n",
    "  def ratio_tls_ssl(self):\n",
    "    tls=0\n",
    "    ssl=0\n",
    "    for key in self.version_of_ssl_dict.keys():\n",
    "      if 'tls' in key.lower():\n",
    "        tls+=self.version_of_ssl_dict[key]\n",
    "      elif 'ssl' in key.lower():\n",
    "        ssl+=self.version_of_ssl_dict[key]\n",
    "    if tls+ssl==0:\n",
    "      return -1\n",
    "    return tls/(tls+ssl)\n",
    "  #feature 15\n",
    "  def ratio_sni(self):\n",
    "    if len(self.ssl_logs_list)!=0:\n",
    "      return self.ssl_with_SNI/len(self.ssl_logs_list)\n",
    "    return -1\n",
    "  def sni_as_ip(self):\n",
    "    return self.SNI_equal_DstIP\n",
    "  def mean_certificate_path(self):\n",
    "    total_path=0\n",
    "    count_certi_path=0\n",
    "    for key in self.certificate_path.keys():\n",
    "      total_path+=self.certificate_path[key]*int(key)\n",
    "      count_certi_path+=self.certificate_path[key]\n",
    "    if count_certi_path!=0:\n",
    "      return total_path/count_certi_path\n",
    "    return -1\n",
    "  def ratio_self_signed_certificate(self):\n",
    "    if len(self.ssl_logs_list)!=0:\n",
    "      return self.self_signed_cert/len(self.ssl_logs_list)\n",
    "    return -1\n",
    "  def public_key_mean(self):\n",
    "    total=0\n",
    "    index=0\n",
    "    for key in self.certificate_key_length_dict.keys():\n",
    "      total+=self.certificate_key_length_dict[key]*int(key)\n",
    "    for key in self.certificate_key_length_dict.keys():\n",
    "      index+=self.certificate_key_length_dict[key]\n",
    "    if index!=0:\n",
    "      return total/index\n",
    "    return -1\n",
    "  def mean_certificate_validatity_periods(self):\n",
    "    if self.certificate_valid_number!=0:\n",
    "      if numpy.mean(self.temp_list)!=self.certificate_valid_length/self.certificate_valid_number:\n",
    "        print(\"error: mean certificate length\")\n",
    "      return self.certificate_valid_length/self.certificate_valid_number\n",
    "    return -1\n",
    "  def standard_deviation_certificate_validatity_periods(self):\n",
    "    if self.certificate_valid_number!=0:\n",
    "      EX=self.certificate_valid_length/self.certificate_valid_number\n",
    "      EX2=self.certificate_valid_length_pow/self.certificate_valid_number\n",
    "      return pow(EX2-(EX*EX), 0.5)\n",
    "    return -1\n",
    "  def validity_certificate_period_during_capturing(self):\n",
    "    return self.not_valid_certificate_number\n",
    "  def mean_age_of_cert(self):\n",
    "    if len(self.cert_percent_validity)!=0:\n",
    "      tmp=0\n",
    "      for i in self.cert_percent_validity:\n",
    "        tmp+=i\n",
    "      return tmp/len(self.cert_percent_validity)\n",
    "    return -1\n",
    "  def amount_of_certificates(self):\n",
    "    return len(self.certificate_serial_dict.keys())\n",
    "  def mean_number_of_domains_in_SAN_DNS(self):\n",
    "    if self.number_san_domains_index!=0:\n",
    "      return self.number_san_domains/self.number_san_domains_index\n",
    "    return -1\n",
    "  def ratio_certificate_records_SSL_records(self):\n",
    "    if len(self.ssl_logs_list)!=0:\n",
    "      return len(self.x509_list)/len(self.ssl_logs_list)\n",
    "    return -1\n",
    "  def sni_in_san_dns(self):\n",
    "    if len(self.is_SNI_in_san_dns)!=0:\n",
    "      for a in self.is_SNI_in_san_dns:\n",
    "        if a==0:\n",
    "          return 0\n",
    "      return 1\n",
    "    return -1\n",
    "  def cn_in_san_dns(self):\n",
    "    if len(self.is_CN_in_SAN_list)!=0:\n",
    "      for a in self.is_CN_in_SAN_list:\n",
    "        if a==0:\n",
    "          return 0\n",
    "      return 1\n",
    "    return -1\n",
    "\n",
    "  #GET METHOD\n",
    "  def get_label_of_connection(self):\n",
    "    if self.malware_label>self.normal_label:\n",
    "      return 'MALWARE'\n",
    "    else:\n",
    "      return 'NORMAL'\n",
    "  def is_malware(self):\n",
    "    if self.malware_label > self.normal_label:\n",
    "      return True\n",
    "  def label_connection_number(self):\n",
    "    if self.is_malware():\n",
    "      return 1\n",
    "    else:\n",
    "      return 0\n",
    "      "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------Check connection log------------------------\n",
      "Done loadding conn.log\n",
      "Writing label to conn.log: /home/hiennnn/Documents/DATN/HTTPS malware/download_datasets/CTU-Malware-Capture-Botnet-54\n",
      "     << Number of lines: 119954\n",
      "<< New file conn_label.log was succesfly created.\n",
      "---------------------Check connection log------------------------\n",
      "Done loadding conn.log\n",
      "Writing label to conn.log: /home/hiennnn/Documents/DATN/HTTPS malware/download_datasets/CTU-Malware-Capture-Botnet-157-1\n",
      "     << Number of lines: 28\n",
      "<< New file conn_label.log was succesfly created.\n",
      "---------------------Check connection log------------------------\n",
      "Done loadding conn.log\n",
      "Writing label to conn.log: /home/hiennnn/Documents/DATN/HTTPS malware/download_datasets/CTU-Normal-27\n",
      "     << Number of lines: 35419\n",
      "<< New file conn_label.log was succesfly created.\n",
      "---------------------Check connection log------------------------\n",
      "Done loadding conn.log\n",
      "Writing label to conn.log: /home/hiennnn/Documents/DATN/HTTPS malware/download_datasets/CTU-Malware-Capture-Botnet-25-2\n",
      "     << Number of lines: 242959\n",
      "<< New file conn_label.log was succesfly created.\n",
      "---------------------Check connection log------------------------\n",
      "Done loadding conn.log\n",
      "Writing label to conn.log: /home/hiennnn/Documents/DATN/HTTPS malware/download_datasets/CTU-Malware-Capture-Botnet-116-1\n",
      "     << Number of lines: 1616\n",
      "<< New file conn_label.log was succesfly created.\n",
      "---------------------Check connection log------------------------\n",
      "Done loadding conn.log\n",
      "Writing label to conn.log: /home/hiennnn/Documents/DATN/HTTPS malware/download_datasets/CTU-Malware-Capture-Botnet-188-4\n",
      "     << Number of lines: 184\n",
      "<< New file conn_label.log was succesfly created.\n",
      "---------------------Check connection log------------------------\n",
      "Done loadding conn.log\n",
      "Writing label to conn.log: /home/hiennnn/Documents/DATN/HTTPS malware/download_datasets/CTU-Malware-Capture-Botnet-163-1\n",
      "     << Number of lines: 318\n",
      "<< New file conn_label.log was succesfly created.\n",
      "---------------------Check connection log------------------------\n",
      "Done loadding conn.log\n",
      "Writing label to conn.log: /home/hiennnn/Documents/DATN/HTTPS malware/download_datasets/CTU-Malware-Capture-Botnet-50\n",
      "     << Number of lines: 167321\n",
      "<< New file conn_label.log was succesfly created.\n",
      "---------------------Check connection log------------------------\n",
      "Done loadding conn.log\n",
      "Writing label to conn.log: /home/hiennnn/Documents/DATN/HTTPS malware/download_datasets/CTU-Malware-Capture-Botnet-112-4\n",
      "     << Number of lines: 196105\n",
      "<< New file conn_label.log was succesfly created.\n",
      "---------------------Check connection log------------------------\n",
      "Done loadding conn.log\n",
      "Writing label to conn.log: /home/hiennnn/Documents/DATN/HTTPS malware/download_datasets/CTU-Malware-Capture-Botnet-120-1\n",
      "     << Number of lines: 345\n",
      "<< New file conn_label.log was succesfly created.\n",
      "---------------------Check connection log------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 78\u001b[0m\n\u001b[1;32m     76\u001b[0m sub_folder_name\u001b[39m=\u001b[39msub_folders[index]\n\u001b[1;32m     77\u001b[0m folder\u001b[39m=\u001b[39m\u001b[39mstr\u001b[39m(folder)\n\u001b[0;32m---> 78\u001b[0m take_label_from_binet_flow(\u001b[39mstr\u001b[39;49m(folder)\u001b[39m+\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m+\u001b[39;49m\u001b[39mstr\u001b[39;49m(sub_folder_name))\n",
      "Cell \u001b[0;32mIn[6], line 70\u001b[0m, in \u001b[0;36mtake_label_from_binet_flow\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtake_label_from_binet_flow\u001b[39m(path):\n\u001b[0;32m---> 70\u001b[0m   flow_array\u001b[39m=\u001b[39mcheck_conn_label(path)\n\u001b[1;32m     71\u001b[0m   write_conn_log(path, flow_array)\n",
      "Cell \u001b[0;32mIn[6], line 41\u001b[0m, in \u001b[0;36mcheck_conn_label\u001b[0;34m(path_to_dataset)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m   error\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m     \n\u001b[0;32m---> 41\u001b[0m   split\u001b[39m=\u001b[39mline\u001b[39m.\u001b[39;49msplit(\u001b[39m'\u001b[39;49m\u001b[39m\t\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     42\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m     src_address\u001b[39m=\u001b[39msplit[\u001b[39m2\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "space = '\\t'\n",
    "def write_conn_log(path, flow_array):\n",
    "  print(\"Writing label to conn.log: \"+ path)\n",
    "  index=0\n",
    "  with open(path+\"/bro/conn_label.log\",'w') as f:\n",
    "    for i in range(len(flow_array)):\n",
    "      f.write(flow_array[i])\n",
    "      index+=1\n",
    "  f.close()\n",
    "  print(\"     << Number of lines:\", index)\n",
    "  print(\"<< New file conn_label.log was succesfly created.\")\n",
    "\n",
    "def check_conn_label(path_to_dataset):\n",
    "  print(\"---------------------Check connection log------------------------\")\n",
    "  infected_ips_list=[]\n",
    "  normal_ips_list=[]\n",
    "  flow_array=[]\n",
    "  ips_file=path_to_dataset+'/ips.log'\n",
    "  malware_label=0\n",
    "  normal_label=0\n",
    "  with open(ips_file) as f:\n",
    "    for line in f:\n",
    "      split=line.split(space)\n",
    "      if 'malware' in split[1]:\n",
    "        infected_ips_list.append(split[0])\n",
    "      elif 'normal' in split[1]:\n",
    "        normal_ips_list.append(split[0])\n",
    "  with open(path_to_dataset+\"/bro/conn.log\") as f:\n",
    "    for line in f:\n",
    "      newline=line\n",
    "      if '#'==line[0]:\n",
    "        if 'fields' in line:\n",
    "          newline = line.strip() + space + \"label\" + \"\\n\"\n",
    "        elif 'types' in line:\n",
    "          newline = line.strip() + space + \"string\" + \"\\n\"\n",
    "        else:\n",
    "          continue\n",
    "        flow_array.append(newline)\n",
    "      else:\n",
    "        error=0     \n",
    "        split=line.split('\t')\n",
    "        try:\n",
    "          src_address=split[2]\n",
    "        except:\n",
    "          print(\"Done load connection file\")\n",
    "        error=0\n",
    "        if src_address in infected_ips_list:\n",
    "          newline=line.strip() + space + \"botnet\" + \"\\n\"\n",
    "          error+=1\n",
    "          malware_label+=1\n",
    "        elif src_address in normal_ips_list:\n",
    "          newline=line.strip() + space + \"normal\" + \"\\n\"\n",
    "          error+=1\n",
    "          normal_label+=1\n",
    "        if error==0:\n",
    "          continue\n",
    "        if error>1:\n",
    "          print(\"Error: SrcAddress has more classes. Program is terminated.\")\n",
    "          break\n",
    "        flow_array.append(newline)\n",
    "    f.close()\n",
    "  print('Done loadding conn.log')\n",
    "  malware_label='#malware label: '+str(malware_label)+'\\n'\n",
    "  normal_label='#normal label: '+str(normal_label)+'\\n'\n",
    "  flow_array.insert(0, malware_label)\n",
    "  flow_array.insert(1, normal_label)\n",
    "  return flow_array\n",
    "\n",
    "def take_label_from_binet_flow(path):\n",
    "  flow_array=check_conn_label(path)\n",
    "  write_conn_log(path, flow_array)\n",
    "\n",
    "folder = os.path.join(os.getcwd(),'download_datasets')\n",
    "sub_folders = [name for name in os.listdir(folder) if os.path.isdir(os.path.join(folder, name))]\n",
    "for index in range(len(sub_folders)):\n",
    "  sub_folder_name=sub_folders[index]\n",
    "  folder=str(folder)\n",
    "  take_label_from_binet_flow(str(folder)+'/'+str(sub_folder_name))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading folder CTU-Malware-Capture-Botnet-54\n",
      "Loading x509 file\n",
      "no start_date.txt\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:55\n",
      "          <<< ssl conn:2467\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-157-1\n",
      "Loading x509 file\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:0\n",
      "          <<< ssl conn:1\n",
      "Saving features\n",
      "Reading folder CTU-Normal-27\n",
      "Loading x509 file\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25643/2677728654.py:185: UserWarning: Parsing dates in %d-%m-%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df = pd.DataFrame({'date': pd.to_datetime([date+' '+time])})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:40\n",
      "          <<< ssl conn:6339\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-25-2\n",
      "Loading x509 file\n",
      "Loading conn file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25643/2677728654.py:185: UserWarning: Parsing dates in %d-%m-%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df = pd.DataFrame({'date': pd.to_datetime([date+' '+time])})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:0\n",
      "          <<< ssl conn:236\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-116-1\n",
      "Loading x509 file\n",
      "no start_date.txt\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:0\n",
      "          <<< ssl conn:7\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-188-4\n",
      "Loading x509 file\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:0\n",
      "          <<< ssl conn:71\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-163-1\n",
      "Loading x509 file\n",
      "no start_date.txt\n",
      "no x509 file\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:0\n",
      "          <<< ssl conn:32\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-50\n",
      "Loading x509 file\n",
      "no start_date.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25643/2677728654.py:185: UserWarning: Parsing dates in %d-%m-%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df = pd.DataFrame({'date': pd.to_datetime([date+' '+time])})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:241\n",
      "          <<< ssl conn:5443\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-112-4\n",
      "Loading x509 file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25643/2677728654.py:185: UserWarning: Parsing dates in %d-%m-%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df = pd.DataFrame({'date': pd.to_datetime([date+' '+time])})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:1092\n",
      "          <<< ssl conn:15089\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-120-1\n",
      "Loading x509 file\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:0\n",
      "          <<< ssl conn:61\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-1\n",
      "Loading x509 file\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:31\n",
      "          <<< ssl conn:921\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-112-1\n",
      "Loading x509 file\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:34\n",
      "          <<< ssl conn:715\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-52\n",
      "Loading x509 file\n",
      "no start_date.txt\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:1\n",
      "          <<< ssl conn:39\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-142-1\n",
      "Loading x509 file\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:0\n",
      "          <<< ssl conn:1457\n",
      "Saving features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25643/2677728654.py:185: UserWarning: Parsing dates in %d-%m-%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df = pd.DataFrame({'date': pd.to_datetime([date+' '+time])})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading folder CTU-Normal-6-filterd\n",
      "Loading x509 file\n",
      "no start_date.txt\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:250\n",
      "          <<< ssl conn:140\n",
      "Saving features\n",
      "Reading folder CTU-Normal-9\n",
      "Loading x509 file\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:0\n",
      "          <<< ssl conn:20\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-69\n",
      "Loading x509 file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25643/2677728654.py:185: UserWarning: Parsing dates in %d-%m-%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df = pd.DataFrame({'date': pd.to_datetime([date+' '+time])})\n",
      "/tmp/ipykernel_25643/2677728654.py:185: UserWarning: Parsing dates in %d-%m-%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df = pd.DataFrame({'date': pd.to_datetime([date+' '+time])})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:22199\n",
      "          <<< ssl conn:35255\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-137-1\n",
      "Loading x509 file\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:2\n",
      "          <<< ssl conn:42\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-17-1\n",
      "Loading x509 file\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:0\n",
      "          <<< ssl conn:25\n",
      "Saving features\n",
      "Reading folder CTU-Normal-33\n",
      "Loading x509 file\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:336\n",
      "          <<< ssl conn:1680\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-31-1\n",
      "Loading x509 file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25643/2677728654.py:185: UserWarning: Parsing dates in %d-%m-%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df = pd.DataFrame({'date': pd.to_datetime([date+' '+time])})\n",
      "/tmp/ipykernel_25643/2677728654.py:185: UserWarning: Parsing dates in %d-%m-%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df = pd.DataFrame({'date': pd.to_datetime([date+' '+time])})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:7\n",
      "          <<< ssl conn:85182\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-51\n",
      "Loading x509 file\n",
      "no start_date.txt\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:241\n",
      "          <<< ssl conn:1096\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-25-1\n",
      "Loading x509 file\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:4\n",
      "          <<< ssl conn:11058\n",
      "Saving features\n",
      "Reading folder CTU-Normal-14\n",
      "Loading x509 file\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:92\n",
      "          <<< ssl conn:2879\n",
      "Saving features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25643/2677728654.py:185: UserWarning: Parsing dates in %d-%m-%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df = pd.DataFrame({'date': pd.to_datetime([date+' '+time])})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading folder CTU-Malware-Capture-Botnet-141-2\n",
      "Loading x509 file\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25643/2677728654.py:185: UserWarning: Parsing dates in %d-%m-%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df = pd.DataFrame({'date': pd.to_datetime([date+' '+time])})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNI as IP:  81.91.11.178  and destination ip is:  81.91.11.178\n",
      "SNI as IP:  81.91.11.178  and destination ip is:  81.91.11.178\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:1757\n",
      "          <<< ssl conn:4420\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-138-1\n",
      "Loading x509 file\n",
      "no start_date.txt\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:0\n",
      "          <<< ssl conn:1\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-35-1\n",
      "Loading x509 file\n",
      "Loading conn file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25643/2677728654.py:185: UserWarning: Parsing dates in %d-%m-%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df = pd.DataFrame({'date': pd.to_datetime([date+' '+time])})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:0\n",
      "          <<< ssl conn:2\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-43\n",
      "Loading x509 file\n",
      "no start_date.txt\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:3\n",
      "          <<< ssl conn:686\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-153-1\n",
      "Loading x509 file\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:1925\n",
      "          <<< ssl conn:1259\n",
      "Saving features\n",
      "Reading folder CTU-Normal-12\n",
      "Loading x509 file\n",
      "no start_date.txt\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:4\n",
      "          <<< ssl conn:93\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-25-4\n",
      "Loading x509 file\n",
      "Loading conn file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25643/2677728654.py:185: UserWarning: Parsing dates in %d-%m-%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df = pd.DataFrame({'date': pd.to_datetime([date+' '+time])})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:2\n",
      "          <<< ssl conn:93\n",
      "Saving features\n",
      "Reading folder CTU-Normal-7\n",
      "Loading x509 file\n",
      "no start_date.txt\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:4\n",
      "          <<< ssl conn:93\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-123-1\n",
      "Loading x509 file\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:0\n",
      "          <<< ssl conn:1\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-162-2\n",
      "Loading x509 file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25643/2677728654.py:185: UserWarning: Parsing dates in %d-%m-%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df = pd.DataFrame({'date': pd.to_datetime([date+' '+time])})\n",
      "/tmp/ipykernel_25643/2677728654.py:185: UserWarning: Parsing dates in %d-%m-%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df = pd.DataFrame({'date': pd.to_datetime([date+' '+time])})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:20620\n",
      "          <<< ssl conn:1250\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-47\n",
      "Loading x509 file\n",
      "no start_date.txt\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:25\n",
      "          <<< ssl conn:575\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-140-2\n",
      "Loading x509 file\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25643/2677728654.py:185: UserWarning: Parsing dates in %d-%m-%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df = pd.DataFrame({'date': pd.to_datetime([date+' '+time])})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:1615\n",
      "          <<< ssl conn:2694\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-44\n",
      "Loading x509 file\n",
      "no start_date.txt\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:32\n",
      "          <<< ssl conn:6370\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-110-1\n",
      "Loading x509 file\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "SNI as IP:  94.198.110.184  and destination ip is:  94.198.110.184\n",
      "SNI as IP:  200.55.194.230  and destination ip is:  200.55.194.230\n",
      "SNI as IP:  94.198.110.184  and destination ip is:  94.198.110.184\n",
      "SNI as IP:  94.198.110.184  and destination ip is:  94.198.110.184\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:12\n",
      "          <<< ssl conn:561\n",
      "Saving features\n",
      "Reading folder CTU-Normal-32\n",
      "Loading x509 file\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:499\n",
      "          <<< ssl conn:10247\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-162-1\n",
      "Loading x509 file\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:0\n",
      "          <<< ssl conn:36\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-129-1\n",
      "Loading x509 file\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:0\n",
      "          <<< ssl conn:2\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-169-3\n",
      "Loading x509 file\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25643/2677728654.py:185: UserWarning: Parsing dates in %d-%m-%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df = pd.DataFrame({'date': pd.to_datetime([date+' '+time])})\n",
      "/tmp/ipykernel_25643/2677728654.py:185: UserWarning: Parsing dates in %d-%m-%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df = pd.DataFrame({'date': pd.to_datetime([date+' '+time])})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:77\n",
      "          <<< ssl conn:4622\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-102\n",
      "Loading x509 file\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:1\n",
      "          <<< ssl conn:9\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-53\n",
      "Loading x509 file\n",
      "no start_date.txt\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25643/2677728654.py:185: UserWarning: Parsing dates in %d-%m-%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df = pd.DataFrame({'date': pd.to_datetime([date+' '+time])})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          <<< not ssl conn:41\n",
      "          <<< ssl conn:434\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-45\n",
      "Loading x509 file\n",
      "no start_date.txt\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:291\n",
      "          <<< ssl conn:1935\n",
      "Saving features\n",
      "Reading folder CTU-Normal-8-1\n",
      "Loading x509 file\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:0\n",
      "          <<< ssl conn:24\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-116-2\n",
      "Loading x509 file\n",
      "no start_date.txt\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:4\n",
      "          <<< ssl conn:9\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-208-2\n",
      "Loading x509 file\n",
      "Loading conn file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25643/2677728654.py:185: UserWarning: Parsing dates in %d-%m-%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df = pd.DataFrame({'date': pd.to_datetime([date+' '+time])})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:0\n",
      "          <<< ssl conn:43\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-111-5\n",
      "Loading x509 file\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:1597\n",
      "          <<< ssl conn:32125\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-188-2\n",
      "Loading x509 file\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:5\n",
      "          <<< ssl conn:111\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-42\n",
      "Loading x509 file\n",
      "no start_date.txt\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:465\n",
      "          <<< ssl conn:1527\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-48\n",
      "Loading x509 file\n",
      "no start_date.txt\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:6\n",
      "          <<< ssl conn:73\n",
      "Saving features\n",
      "Reading folder CTU-Normal-25\n",
      "Loading x509 file\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:13\n",
      "          <<< ssl conn:874\n",
      "Saving features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25643/2677728654.py:185: UserWarning: Parsing dates in %d-%m-%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df = pd.DataFrame({'date': pd.to_datetime([date+' '+time])})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading folder CTU-Malware-Capture-Botnet-49\n",
      "Loading x509 file\n",
      "no start_date.txt\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:202\n",
      "          <<< ssl conn:3309\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-25-3\n",
      "Loading x509 file\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:0\n",
      "          <<< ssl conn:3\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-112-2\n",
      "Loading x509 file\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:6\n",
      "          <<< ssl conn:418\n",
      "Saving features\n",
      "Reading folder CTU-Normal-31\n",
      "Loading x509 file\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:166\n",
      "          <<< ssl conn:9674\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-25-5\n",
      "Loading x509 file\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:2\n",
      "          <<< ssl conn:1627\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-145-1\n",
      "Loading x509 file\n",
      "Loading conn file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25643/2677728654.py:185: UserWarning: Parsing dates in %d-%m-%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df = pd.DataFrame({'date': pd.to_datetime([date+' '+time])})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:1\n",
      "          <<< ssl conn:442\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-111-1\n",
      "Loading x509 file\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "SNI as IP:  94.198.110.184  and destination ip is:  94.198.110.184\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:19\n",
      "          <<< ssl conn:338\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-143-1\n",
      "Loading x509 file\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:0\n",
      "          <<< ssl conn:1\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-25-6\n",
      "Loading x509 file\n",
      "Loading conn file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25643/2677728654.py:185: UserWarning: Parsing dates in %d-%m-%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df = pd.DataFrame({'date': pd.to_datetime([date+' '+time])})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:0\n",
      "          <<< ssl conn:4871\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-90\n",
      "Loading x509 file\n",
      "no start_date.txt\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:0\n",
      "          <<< ssl conn:1\n",
      "Saving features\n",
      "Reading folder CTU-Normal-30\n",
      "Loading x509 file\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:61\n",
      "          <<< ssl conn:8545\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-27-1\n",
      "Loading x509 file\n",
      "Loading conn file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25643/2677728654.py:185: UserWarning: Parsing dates in %d-%m-%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df = pd.DataFrame({'date': pd.to_datetime([date+' '+time])})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:0\n",
      "          <<< ssl conn:763\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-144-1\n",
      "Loading x509 file\n",
      "Loading conn file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25643/2677728654.py:185: UserWarning: Parsing dates in %d-%m-%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df = pd.DataFrame({'date': pd.to_datetime([date+' '+time])})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:0\n",
      "          <<< ssl conn:42\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-17-2\n",
      "Loading x509 file\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:0\n",
      "          <<< ssl conn:24\n",
      "Saving features\n",
      "Reading folder CTU-Normal-23\n",
      "Loading x509 file\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25643/2677728654.py:185: UserWarning: Parsing dates in %d-%m-%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df = pd.DataFrame({'date': pd.to_datetime([date+' '+time])})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:43\n",
      "          <<< ssl conn:3076\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-141-1\n",
      "Loading x509 file\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25643/2677728654.py:185: UserWarning: Parsing dates in %d-%m-%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df = pd.DataFrame({'date': pd.to_datetime([date+' '+time])})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:1598\n",
      "          <<< ssl conn:3616\n",
      "Saving features\n",
      "Reading folder CTU-Normal-24\n",
      "Loading x509 file\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:9\n",
      "          <<< ssl conn:1098\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-61-1\n",
      "Loading x509 file\n",
      "Loading conn file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25643/2677728654.py:185: UserWarning: Parsing dates in %d-%m-%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df = pd.DataFrame({'date': pd.to_datetime([date+' '+time])})\n",
      "/tmp/ipykernel_25643/2677728654.py:185: UserWarning: Parsing dates in %d-%m-%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df = pd.DataFrame({'date': pd.to_datetime([date+' '+time])})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:833\n",
      "          <<< ssl conn:823\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-27-2\n",
      "Loading x509 file\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:0\n",
      "          <<< ssl conn:4\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-219-2\n",
      "Loading x509 file\n",
      "Loading conn file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25643/2677728654.py:185: UserWarning: Parsing dates in %d-%m-%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df = pd.DataFrame({'date': pd.to_datetime([date+' '+time])})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:4\n",
      "          <<< ssl conn:723\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-116-4\n",
      "Loading x509 file\n",
      "no start_date.txt\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:21\n",
      "          <<< ssl conn:36\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-46\n",
      "Loading x509 file\n",
      "no start_date.txt\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:16\n",
      "          <<< ssl conn:344\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-140-1\n",
      "Loading x509 file\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:1028\n",
      "          <<< ssl conn:2859\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-78-1\n",
      "Loading x509 file\n",
      "Loading conn file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25643/2677728654.py:185: UserWarning: Parsing dates in %d-%m-%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df = pd.DataFrame({'date': pd.to_datetime([date+' '+time])})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:98\n",
      "          <<< ssl conn:70362\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-110-2\n",
      "Loading x509 file\n",
      "Loading conn file\n",
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:0\n",
      "          <<< ssl conn:55\n",
      "Saving features\n",
      "Reading folder CTU-Malware-Capture-Botnet-78-2\n",
      "Loading x509 file\n",
      "Loading conn file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25643/2677728654.py:185: UserWarning: Parsing dates in %d-%m-%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df = pd.DataFrame({'date': pd.to_datetime([date+' '+time])})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Createing connection 4-tuple\n",
      "adding conn record not belong to aggregation\n",
      "          <<< not ssl conn:9\n",
      "          <<< ssl conn:28584\n",
      "Saving features\n"
     ]
    }
   ],
   "source": [
    "class EvaluateData():\n",
    "  space='\\t'\n",
    "  def __init__(self):\n",
    "    #self.name_of_result=name_of_result\n",
    "    self.connection_4_tuples=dict() #connection 4-tuple in dataset\n",
    "    self.x509_dict=dict() #list x509 record in dataset\n",
    "    self.conn_dict=dict()#list connection record in dataset\n",
    "    self.all_conn_dict=dict()\n",
    "\n",
    "    self.control_ssl_uids_dict=dict() #list uids in all connection 4-tuples\n",
    "    self.not_added_x509=0 #aggreation not have x509\n",
    "    self.is_computed_433feature=False\n",
    "    self.number_ssl_logs=0 \n",
    "    self.number_x509_logs=0\n",
    "\n",
    "  def evaluate_features(self, path_to_dataset):\n",
    "    self.load_x509_file(path_to_dataset)\n",
    "    self.load_conn_file(path_to_dataset)\n",
    "    self.create_4_tuples(path_to_dataset)\n",
    "    self.add_not_ssl_logs(path_to_dataset)\n",
    "    return self.save_features(path_to_dataset)\n",
    "\n",
    "#\n",
    "  def create_4_tuples(self, path_to_dataset): #create 4-tuple for 1 dataset\n",
    "    print('Createing connection 4-tuple')\n",
    "    background_flows=0\n",
    "    number_of_adding_x509=0\n",
    "    number_adding_ssl=0 #number aggreation in dataset \n",
    "    count_lines=0\n",
    "    conn_split=[] \n",
    "\n",
    "    self.not_added_x509=0\n",
    "    #open each ssl file in dataset\n",
    "    with open(path_to_dataset+\"/bro/ssl.log\") as ssl_file:\n",
    "      for ssl_line in ssl_file:\n",
    "        if '#' == ssl_line[0]:\n",
    "          continue\n",
    "        count_lines+=1\n",
    "        ssl_split=ssl_line.split('\t')\n",
    "        ssl_uid=ssl_split[1]\n",
    "        try:\n",
    "          if self.control_ssl_uids_dict[ssl_uid]:\n",
    "            continue\n",
    "        except:\n",
    "          self.control_ssl_uids_dict[ssl_uid]=1\n",
    "        try:\n",
    "          conn_log=self.conn_dict[ssl_uid]\n",
    "          conn_split=conn_log.split('\t')\n",
    "        except:\n",
    "          continue\n",
    "        connection_index=conn_split[2], conn_split[4], conn_split[5], conn_split[6]\n",
    "        try:\n",
    "          label=conn_split[21]\n",
    "        except IndexError:\n",
    "          print(\"Error: no label in conn line\")\n",
    "        label=label.lower()\n",
    "        if 'background' in label or 'No_Label' in label:\n",
    "          background_flows+=1\n",
    "          print('Error: background flows')\n",
    "          continue\n",
    "        \n",
    "        if not ('botnet' in label) and not('normal' in label):\n",
    "          print(\"Error: more state of label\")\n",
    "          continue\n",
    "\n",
    "        #add conn_log to conenction 4-tuple\n",
    "        try: \n",
    "          #add conn record to exist Connection 4-tuple\n",
    "          self.connection_4_tuples[connection_index].add_ssl_flow(conn_log, label) #caculate connection feature\n",
    "        except:\n",
    "          #create new Connection 4-tuple, add conn record to Connection 4-tuple\n",
    "          self.connection_4_tuples[connection_index]=Connection4tuple(connection_index)\n",
    "          self.connection_4_tuples[connection_index].add_ssl_flow(conn_log, label)\n",
    "        #add x509 record to SSL aggregation \n",
    "        valid_x509_list=self.split_ssl(ssl_line, connection_index, label) #get first x059 from ssl record\n",
    "        number_of_adding_x509+=len(valid_x509_list)\n",
    "        self.connection_4_tuples[connection_index].add_ssl_log(ssl_line, valid_x509_list, os.path.basename(path_to_dataset))\n",
    "        number_adding_ssl+=1 \n",
    "        self.number_ssl_logs+=1\n",
    "        self.number_x509_logs+=len(valid_x509_list)\n",
    "      ssl_file.close()\n",
    "    self.conn_dict=dict()\n",
    "    self.x509_dict=dict()\n",
    "    self.control_ssl_uids_dict=dict()\n",
    "    self.count_statistic_of_conn(count_lines, background_flows, number_adding_ssl, number_of_adding_x509)\n",
    "\n",
    "  def split_ssl(self, ssl_line, tuple_index, label):\n",
    "    split=ssl_line.split('\t')\n",
    "\n",
    "    if '-'==split[14] or '(object)'==split[14]:\n",
    "      self.not_added_x509+=1\n",
    "      return []\n",
    "    #self.put_server_name_to_dict(split[1], split[9], tuple_index, split[14], label)\n",
    "    return [self.get_x509_line(split[14].split(','))]\n",
    "  #def put_server_name_to_dict(self, ssl_uid, server):\n",
    "  def get_x509_line(self, x509_uids_list):\n",
    "    x509_line=None\n",
    "    uid_x509=x509_uids_list[0]\n",
    "    try:\n",
    "      if self.x509_dict[uid_x509]:\n",
    "        x509_line=self.x509_dict[uid_x509][0]\n",
    "        if len(self.x509_dict[uid_x509])>1:\n",
    "          print(\"Error:Error: [ProcessLogs] Actual ssl flow needs x509 log, which has more same uids!!!!\")\n",
    "    except:\n",
    "      print(\"Error: [get_x509_lines] In ProcessLogs.py x509 does not have this x509uid: \"+x509_uids_list[0])\n",
    "    return x509_line\n",
    "\n",
    "  def count_statistic_of_conn(self, number_of_lines, background_flows, number_adding_ssl, number_adding_x509):\n",
    "    malware_tuple=0\n",
    "    normal_tuple=0\n",
    "    malware_flows=0\n",
    "    normal_flows=0\n",
    "    for key in self.connection_4_tuples.keys():\n",
    "      if int(len(self.connection_4_tuples[key].uid_flow_dict))!=int(self.connection_4_tuples[key].number_of_connection_records()):\n",
    "        print(\"Error: connection uid and number of connection not same\")\n",
    "      if self.connection_4_tuples[key].is_malware():\n",
    "        malware_tuple+=1\n",
    "        malware_flows += self.connection_4_tuples[key].number_of_connection_records()\n",
    "      else:\n",
    "        normal_tuple+=1\n",
    "        normal_flows += self.connection_4_tuples[key].number_of_connection_records()\n",
    "    #print(malware_tuple, normal_tuple)\n",
    "\n",
    "#loading, add all connection record to conn_dict\n",
    "  def load_conn_file(self, path_to_dataset):\n",
    "    print('Loading conn file')\n",
    "    space='\\t'\n",
    "    file = open(path_to_dataset+'/bro/conn_label1.log', 'r')\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "      if '#' ==line[0]:\n",
    "          continue\n",
    "      split=line.split(space)\n",
    "      conn_uid=split[1]\n",
    "      label=split[21]\n",
    "      if 'background' in label or 'no_label' in label:\n",
    "        continue\n",
    "      try:\n",
    "        if self.conn_dict[conn_uid]:\n",
    "          print(\"Error: More conn log with same uid\")\n",
    "      except:\n",
    "        self.conn_dict[conn_uid]=line\n",
    "\n",
    "\n",
    "#edit datetime in x509.log\n",
    "  def load_x509_file(self, path_to_dataset):\n",
    "    print('Loading x509 file')\n",
    "    started_unix_time=0.0\n",
    "    try:\n",
    "      path=path_to_dataset + '/start_date.txt'\n",
    "      with open(path) as f:\n",
    "        started_date = f.readline().split(',')\n",
    "        utc_date=self.from_cet_to_utc(started_date)\n",
    "        started_unix_time=self.from_utc_to_timestamp(utc_date)\n",
    "        #print(started_unix_time)\n",
    "\n",
    "    except:\n",
    "      print('no start_date.txt')\n",
    "    \n",
    "    try:\n",
    "      with open(path_to_dataset+'/bro/x509.log') as f:\n",
    "        for line in f:\n",
    "          if '#' ==line[0]:\n",
    "            continue\n",
    "          split=line.split('\\t')\n",
    "          time_new=float(split[0])+started_unix_time\n",
    "          new_line=str(time_new)\n",
    "          for i in range(1, len(split)):\n",
    "            new_line+='\\t'+split[i]\n",
    "          x509_uid=split[1]\n",
    "          try:\n",
    "            self.x509_dict[x509_uid].append(new_line)\n",
    "          except:\n",
    "            self.x509_dict[x509_uid]=[]\n",
    "            self.x509_dict[x509_uid].append(new_line)\n",
    "        f.close()\n",
    "    except IOError:\n",
    "      print(\"no x509 file\")\n",
    "      \n",
    "  def from_cet_to_utc(self,started_date):\n",
    "    date=started_date[0].split('/')\n",
    "    time=started_date[1].strip().rstrip().split(':')\n",
    "    date='-'.join(map(str, date))\n",
    "    time=':'.join(map(str, time))\n",
    "    df = pd.DataFrame({'date': pd.to_datetime([date+' '+time])})\n",
    "    df=df['date'].dt.tz_localize('Europe/Brussels').dt.tz_convert('UTC').dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    return df[0]\n",
    "  def from_utc_to_timestamp(self,date):\n",
    "    datetime_object = datetime.strptime(date, '%Y-%m-%d %H:%M:%S')\n",
    "    return datetime_object.timestamp()\n",
    "\n",
    "#add conn record not in aggregation to Connection 4-tuple\n",
    "  def add_not_ssl_logs(self, path_to_dataset):\n",
    "    print(\"adding conn record not belong to aggregation\")\n",
    "    not_ssl_conn=0\n",
    "    space='\\t'\n",
    "    ssl_conn=0\n",
    "    tmp=0\n",
    "    \n",
    "    with open(path_to_dataset+'/bro/conn_label1.log') as f:\n",
    "      for line in f:\n",
    "        if '#'==line[0]:\n",
    "          continue\n",
    "        conn_split=line.split(space)\n",
    "        connection_index=conn_split[2], conn_split[4], conn_split[5], conn_split[6]\n",
    "        try:\n",
    "          label=conn_split[21]\n",
    "        except IndexError:\n",
    "          continue\n",
    "        conn_uid=conn_split[1]\n",
    "        if 'Background' in label or 'No_Label' in label:\n",
    "          continue\n",
    "        if connection_index in self.connection_4_tuples:\n",
    "          tmp+=1\n",
    "          if conn_uid in self.connection_4_tuples[connection_index].uid_flow_dict:\n",
    "            ssl_conn+=1\n",
    "          else:\n",
    "            self.connection_4_tuples[connection_index].add_not_ssl_flow(line, label)\n",
    "            not_ssl_conn += 1\n",
    "        else:\n",
    "          continue\n",
    "          #print(conn_uid)\n",
    "\n",
    "    f.close()\n",
    "    print(\"          <<< not ssl conn:\"+str(not_ssl_conn))\n",
    "    print(\"          <<< ssl conn:\"+str(ssl_conn))\n",
    "              \n",
    "  def save_features(self, path_to_dataset):\n",
    "    print('Saving features')\n",
    "    data=[]\n",
    "    for key in self.connection_4_tuples.keys():      \n",
    "      d=dict()\n",
    "      d['connection_index']=key\n",
    "      d['dataset']=path_to_dataset.split('/')[-1]\n",
    "      d['number_of_connection_records']=self.connection_4_tuples[key].number_of_connection_records()\n",
    "      d['mean_of_duration']=self.connection_4_tuples[key].mean_of_duration()\n",
    "      d['standard_deviation_of_duration']=self.connection_4_tuples[key].standard_deviation_of_duration()\n",
    "      d['standard_deviation_range_duration']=self.connection_4_tuples[key].standard_deviation_range_duration()\n",
    "      d['payload_bytes_from_originator']=self.connection_4_tuples[key].payload_bytes_from_originator()\n",
    "      d['payload_bytes_from_responder']=self.connection_4_tuples[key].payload_bytes_from_responder()\n",
    "      d['ratio_of_responder_bytes']=self.connection_4_tuples[key].ratio_of_responder_bytes()\n",
    "      d['ratio_established_states']=self.connection_4_tuples[key].ratio_established_states()\n",
    "      d['inbound_packets']=self.connection_4_tuples[key].get_inbound_packets()\n",
    "      d['outbound_packets']=self.connection_4_tuples[key].get_outbound_packets()\n",
    "      d['periodicity_mean']=self.connection_4_tuples[key].periodicity_mean()\n",
    "      d['standart_deviation_of_periodicity']=self.connection_4_tuples[key].standart_deviation_of_periodicity()\n",
    "      d['ratio_connection_record_ssl_aggregations']=self.connection_4_tuples[key].ratio_connection_record_ssl_aggregations()\n",
    "      d['ratio_tls_ssl']=self.connection_4_tuples[key].ratio_tls_ssl()\n",
    "      d['ratio_sni']=self.connection_4_tuples[key].ratio_sni()\n",
    "      d['sni_as_ip']=self.connection_4_tuples[key].sni_as_ip()\n",
    "      d['mean_certificate_path']=self.connection_4_tuples[key].mean_certificate_path()\n",
    "      d['ratio_self_signed_certificate']=self.connection_4_tuples[key].ratio_self_signed_certificate()\n",
    "      d['public_key_mean']=self.connection_4_tuples[key].public_key_mean()\n",
    "      d['mean_certificate_validatity_periods']=self.connection_4_tuples[key].mean_certificate_validatity_periods()\n",
    "      d['standard_deviation_certificate_validatity_periods']=self.connection_4_tuples[key].standard_deviation_certificate_validatity_periods()\n",
    "      d['validity_certificate_period_during_capturing']=self.connection_4_tuples[key].validity_certificate_period_during_capturing()\n",
    "      d['mean_age_of_cert']=self.connection_4_tuples[key].mean_age_of_cert()\n",
    "      d['amount_of_certificates']=self.connection_4_tuples[key].amount_of_certificates()\n",
    "      d['mean_number_of_domains_in_SAN_DNS']=self.connection_4_tuples[key].mean_number_of_domains_in_SAN_DNS()\n",
    "      d['ratio_certificate_records_SSL_records']=self.connection_4_tuples[key].ratio_certificate_records_SSL_records()\n",
    "      d['sni_in_san_dns']=self.connection_4_tuples[key].sni_in_san_dns()\n",
    "      d['cn_in_san_dns']=self.connection_4_tuples[key].cn_in_san_dns()\n",
    "      d['label']=self.connection_4_tuples[key].label_connection_number()\n",
    "      data.append(d)\n",
    "    return data\n",
    "    \n",
    "folder = os.getcwd()\n",
    "file_path=os.path.join(folder,\"datasets/extract_dataset_before.csv\")\n",
    "folder = os.path.join(folder,'download_datasets')\n",
    "sub_folders = [name for name in os.listdir(folder) if os.path.isdir(os.path.join(folder, name))]\n",
    "dataset=pd.DataFrame()\n",
    "for index in range(0, len(sub_folders),1):\n",
    "    print(\"Reading folder \"+sub_folders[index])\n",
    "    sub_folder_name=os.path.join(folder,sub_folders[index])\n",
    "    process_log=EvaluateData()\n",
    "    data=process_log.evaluate_features(sub_folder_name)\n",
    "    dataset = pd.concat([dataset, pd.DataFrame(data)], ignore_index=True)\n",
    "dataset.to_csv(file_path, mode='w', header=True, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature number_of_connection_records: P-value =  Chi2-statistic = 0.013374823463316199\n",
      "Feature mean_of_duration: P-value =  Chi2-statistic = 0.05129462926197048\n",
      "Feature standard_deviation_of_duration: P-value =  Chi2-statistic = 0.03880061602316753\n",
      "Feature standard_deviation_range_duration: P-value =  Chi2-statistic = 0.011212812228883928\n",
      "Feature payload_bytes_from_originator: P-value =  Chi2-statistic = 0.0767468712870012\n",
      "Feature payload_bytes_from_responder: P-value =  Chi2-statistic = 0.08349061132137314\n",
      "Feature ratio_of_responder_bytes: P-value =  Chi2-statistic = 0.05350313732680778\n",
      "Feature ratio_established_states: P-value =  Chi2-statistic = 0.008053710304106731\n",
      "Feature inbound_packets: P-value =  Chi2-statistic = 0.06391892317483627\n",
      "Feature outbound_packets: P-value =  Chi2-statistic = 0.0459770805479669\n",
      "Feature periodicity_mean: P-value =  Chi2-statistic = 0.0557322424595903\n",
      "Feature standart_deviation_of_periodicity: P-value =  Chi2-statistic = 0.08849956481088861\n",
      "Feature ratio_connection_record_ssl_aggregations: P-value =  Chi2-statistic = 0.06136306024723903\n",
      "Feature ratio_tls_ssl: P-value =  Chi2-statistic = 0.038277765104480554\n",
      "Feature ratio_sni: P-value =  Chi2-statistic = 0.05703995769271675\n",
      "Feature sni_as_ip: P-value =  Chi2-statistic = 0.006266776094939752\n",
      "Feature mean_certificate_path: P-value =  Chi2-statistic = 0.07736043843354845\n",
      "Feature ratio_self_signed_certificate: P-value =  Chi2-statistic = 0.0\n",
      "Feature public_key_mean: P-value =  Chi2-statistic = 0.04559055825690406\n",
      "Feature mean_certificate_validatity_periods: P-value =  Chi2-statistic = 0.25082557709722075\n",
      "Feature standard_deviation_certificate_validatity_periods: P-value =  Chi2-statistic = 0.03498404956711609\n",
      "Feature validity_certificate_period_during_capturing: P-value =  Chi2-statistic = 0.0015519455033126128\n",
      "Feature mean_age_of_cert: P-value =  Chi2-statistic = 0.2518380651481271\n",
      "Feature amount_of_certificates: P-value =  Chi2-statistic = 0.03817655164078659\n",
      "Feature mean_number_of_domains_in_SAN_DNS: P-value =  Chi2-statistic = 0.12733883802411006\n",
      "Feature ratio_certificate_records_SSL_records: P-value =  Chi2-statistic = 0.034589816878402235\n",
      "Feature sni_in_san_dns: P-value =  Chi2-statistic = 0.04810723447554688\n",
      "Feature cn_in_san_dns: P-value =  Chi2-statistic = 0.034084968862768905\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import  mutual_info_classif\n",
    "\n",
    "# Load the dataset\n",
    "folder = os.path.join(os.getcwd(),'datasets')\n",
    "file_path=os.path.join(folder,\"extract_dataset_before.csv\")\n",
    "df = pd.read_csv(file_path)\n",
    "df=df.iloc[:,2:]\n",
    "\n",
    "# # Split the dataset into features and target variable\n",
    "X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1:]\n",
    "\n",
    "# # Apply the Chi2 method for feature selection\n",
    "scores = mutual_info_classif(X, y)\n",
    "\n",
    "for i in range(len(scores)):\n",
    "    print(f\"Feature {X.columns[i]}: P-value =  Chi2-statistic = {scores[i]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(root_folder):\n",
    "    folder = os.path.join(root_folder,'datasets')\n",
    "    dataset=pd.read_csv(os.path.join(folder,'extract_dataset_after.csv'))\n",
    "    data=dataset.iloc[:,:-1]\n",
    "    label=dataset.iloc[:,-1]\n",
    "    X_train, X_test, y_train, y_test = train_test_split( data, label, test_size=0.2, random_state=42, stratify=label)\n",
    "    train=pd.concat([X_train, y_train], axis=1, join='inner')\n",
    "    test=pd.concat([X_test, y_test], axis=1, join='inner')\n",
    "    train=pd.DataFrame(train)\n",
    "    test=pd.DataFrame(test)\n",
    "    normalizer = preprocessing.MaxAbsScaler()\n",
    "    train.iloc[:,2:-1] = normalizer.fit_transform(train.iloc[:,2:-1])\n",
    "    for column in train.iloc[:,2:-1].columns.values:\n",
    "        train.loc[train[column] <0, column] = -1\n",
    "    scaler_filename = os.path.join(root_folder,\"scaler.save\")\n",
    "    joblib.dump(normalizer, scaler_filename) \n",
    "    test.iloc[:,2:-1] = normalizer.transform(test.iloc[:,2:-1])\n",
    "    for column in test.iloc[:,2:-1].columns.values:\n",
    "        test.loc[test[column] <0, column] = -1\n",
    "    train.to_csv(os.path.join(folder,'trainSet.csv'), index=False, index_label=False)\n",
    "    test.to_csv(os.path.join(folder,'testSet.csv'), index=False, index_label=False)\n",
    "   \n",
    "split_dataset(os.getcwd())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder=os.getcwd()\n",
    "dataset=os.path.join(folder,'datasets')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1668  245]\n",
      " [ 528 1244]]\n",
      "-------------------------\n",
      "Accuracy: 0.7902306648575306\n",
      "Precision score:  0.8354600402955004\n",
      "Recall score:  0.7020316027088036\n",
      "F1 score:  0.7629561484207298\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.87      0.81      1913\n",
      "           1       0.84      0.70      0.76      1772\n",
      "\n",
      "    accuracy                           0.79      3685\n",
      "   macro avg       0.80      0.79      0.79      3685\n",
      "weighted avg       0.80      0.79      0.79      3685\n",
      "\n",
      "-------------------------\n",
      "auc= 0.7869802550919868\n"
     ]
    }
   ],
   "source": [
    "\n",
    "testSet=pd.read_csv(os.path.join(dataset,'testSet.csv'))\n",
    "trainSet=pd.read_csv(os.path.join(dataset,'trainSet.csv'))\n",
    "trainLabel=trainSet.iloc[:,-1]\n",
    "trainData=trainSet.iloc[:,2:-1] \n",
    "testLabel=testSet.iloc[:,-1]\n",
    "testData=testSet.iloc[:,2:-1]\n",
    "kernels= ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "svm_C=110\n",
    "sv_gamma=0.1\n",
    "\n",
    "clf = svm.SVC(kernel=kernels[2], C=svm_C, gamma=sv_gamma)\n",
    "\n",
    "clf.fit(trainData, trainLabel)\n",
    "y_pred=clf.predict(testData)\n",
    "\n",
    "print(confusion_matrix(testLabel, y_pred))\n",
    "print('-------------------------')\n",
    "print(\"Accuracy:\",metrics.accuracy_score(testLabel, y_pred))\n",
    "print(\"Precision score: \",metrics.precision_score(testLabel, y_pred))\n",
    "print(\"Recall score: \",metrics.recall_score(testLabel, y_pred))\n",
    "print(\"F1 score: \",metrics.f1_score(testLabel, y_pred))\n",
    "print(classification_report(testLabel, y_pred))\n",
    "print('-------------------------')\n",
    "print('auc=',metrics.roc_auc_score(testLabel, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1822   91]\n",
      " [ 178 1594]]\n",
      "-------------------------\n",
      "Accuracy: 0.9270013568521032\n",
      "Precision score:  0.9459940652818991\n",
      "Recall score:  0.899548532731377\n",
      "F1 score:  0.9221868672259185\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93      1913\n",
      "           1       0.95      0.90      0.92      1772\n",
      "\n",
      "    accuracy                           0.93      3685\n",
      "   macro avg       0.93      0.93      0.93      3685\n",
      "weighted avg       0.93      0.93      0.93      3685\n",
      "\n",
      "-------------------------\n",
      "auc= 0.9259896348967914\n"
     ]
    }
   ],
   "source": [
    "testSet=pd.read_csv(os.path.join(dataset,'testSet.csv'))\n",
    "trainSet=pd.read_csv(os.path.join(dataset,'trainSet.csv'))\n",
    "trainLabel=trainSet.iloc[:,-1]\n",
    "trainData=trainSet.iloc[:,2:-1] \n",
    "testLabel=testSet.iloc[:,-1]\n",
    "testData=testSet.iloc[:,2:-1] \n",
    "\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(trainData,trainLabel)\n",
    "\n",
    "y_pred=clf.predict(testData)\n",
    "\n",
    "print(confusion_matrix(testLabel, y_pred))\n",
    "print('-------------------------')\n",
    "print(\"Accuracy:\",metrics.accuracy_score(testLabel, y_pred))\n",
    "print(\"Precision score: \",metrics.precision_score(testLabel, y_pred))\n",
    "print(\"Recall score: \",metrics.recall_score(testLabel, y_pred))\n",
    "print(\"F1 score: \",metrics.f1_score(testLabel, y_pred))\n",
    "print(classification_report(testLabel, y_pred))\n",
    "print('-------------------------')\n",
    "print('auc=',metrics.roc_auc_score(testLabel, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1838   75]\n",
      " [ 102 1670]]\n",
      "-------------------------\n",
      "Accuracy: 0.9519674355495251\n",
      "Precision score:  0.9570200573065902\n",
      "Recall score:  0.9424379232505643\n",
      "F1 score:  0.9496730167756611\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.95      1913\n",
      "           1       0.96      0.94      0.95      1772\n",
      "\n",
      "    accuracy                           0.95      3685\n",
      "   macro avg       0.95      0.95      0.95      3685\n",
      "weighted avg       0.95      0.95      0.95      3685\n",
      "\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "testSet=pd.read_csv(os.path.join(dataset,'testSet.csv'))\n",
    "trainSet=pd.read_csv(os.path.join(dataset,'trainSet.csv'))\n",
    "trainLabel=trainSet.iloc[:,-1]\n",
    "trainData=trainSet.iloc[:,2:-1] \n",
    "testLabel=testSet.iloc[:,-1]\n",
    "testData=testSet.iloc[:,2:-1]\n",
    "xgb_classifier = xgb.XGBClassifier(n_estimators=1000, max_depth=10, gamma=0, min_child_weight=1)\n",
    "# xgb_classifier = xgb.XGBClassifier(n_estimators=1000, max_depth=3, gamma=0.1, min_child_weight=5)\n",
    "xgb_classifier.fit(trainData,trainLabel)\n",
    "y_pred = xgb_classifier.predict(testData)\n",
    "\n",
    "print(confusion_matrix(testLabel, y_pred))\n",
    "print('-------------------------')\n",
    "print(\"Accuracy:\",metrics.accuracy_score(testLabel, y_pred))\n",
    "print(\"Precision score: \",metrics.precision_score(testLabel, y_pred))\n",
    "print(\"Recall score: \",metrics.recall_score(testLabel, y_pred))\n",
    "print(\"F1 score: \",metrics.f1_score(testLabel, y_pred))\n",
    "print(classification_report(testLabel, y_pred))\n",
    "print('-------------------------')\n",
    "xgb_classifier.save_model(os.path.join(folder,\"model_xgb.bin\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
